{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ff40f-912a-4f37-965f-5073646c5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e3a474-1bee-4a54-a99c-b148e3f9c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q kaggle\n",
    "!mkdir -p .kaggle\n",
    "!cp \"./kaggle.json\" .kaggle/\n",
    "!chmod 600 .kaggle/kaggle.json\n",
    "!cp -r .kaggle /root\n",
    "\n",
    "!kaggle -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d510233c-6cb6-4b72-b448-67df42254594",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install iterative-stratification==0.1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726d1f0-5810-41f3-a470-53c4148d4679",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls pseudo_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad7e4d-4f94-44fe-bc50-596ce43826c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle kernels output takamichitoda/fb3-pseudo-train-3rd\n",
    "#!rm -rf microsoft-deberta-v3-base_seed2_fold1_best.pth\n",
    "\n",
    "#!kaggle kernels output takamichitoda/fb3-pseudo-train-3rd-fold-1-only\n",
    "#!rm -rf fb3-pseudo-train-3rd-fold-1-only.log\n",
    "\n",
    "#!mkdir -p pseudo_base\n",
    "#!mv microsoft-deberta-v3-base_seed* pseudo_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba5982e-0360-4d5c-83b8-b6796d6c5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle competitions download -c feedback-prize-english-language-learning\n",
    "#!unzip feedback-prize-english-language-learning.zip\n",
    "#!mkdir feedback-prize-english-language-learning\n",
    "#!mv sample_submission.csv test.csv train.csv feedback-prize-english-language-learning/\n",
    "#!rm -rf feedback-prize-english-language-learning.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f69eb-7e9a-4d47-9344-625ed93cb33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir fb3-make-pseudo-label-4th\n",
    "#!kaggle kernels output takamichitoda/fb3-make-pseudo-label-4th -p fb3-make-pseudo-label-4th/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c275d2-4f3f-4e61-a6ae-2d9ffe632ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n",
      "cuda\n",
      "transformers.__version__: 4.20.1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "is_gpu = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_gpu else 'cpu')\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=is_gpu)\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "print(device)\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff99732e-7792-470a-b49d-acfc0dfdfb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    EXP = \"exp15x\"\n",
    "    INPUT = \"/home/jupyter/feedback-prize-english-language-learning\"\n",
    "    OUTPUT = f\"/home/jupyter/{EXP}/\"\n",
    "    SEED = 0\n",
    "    N_FOLD = 4\n",
    "    TARGETS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    \n",
    "    #MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "    MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "    TOKENIZER = None\n",
    "    MAX_LEN = 1428\n",
    "    GRAD_CHECKPOINT = True\n",
    "    \n",
    "    N_EPOCH = 4\n",
    "    N_WORKER = 2\n",
    "    ENCODER_LR = 5e-6\n",
    "    DECODER_LR = 1e-4\n",
    "    EPS = 1e-6\n",
    "    BETAS = (0.9, 0.999)\n",
    "    WEIGHT_DECAY = 0.1\n",
    "    N_WARMUP = 0\n",
    "    N_CYCLES = 0.5\n",
    "    \n",
    "    BS = 8\n",
    "    ACCUMLATION = 1\n",
    "    \n",
    "    GRAD_NORM = 0.01\n",
    "    \n",
    "    ADV_EPS = 1e-4\n",
    "    ADV_LR = 1e-4\n",
    "    ADV_START = 2\n",
    "    \n",
    "    FGM_EPS = 1e-1\n",
    "    FGM_END = float(\"inf\")\n",
    "    \n",
    "    SKIP_FOLDS = [1,2,3]\n",
    "    LOCAL_SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32370e8d-48f2-4f0b-a329-b8ec14301544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>The best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>Small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n",
       "3  003885A45F42  The best time in life is when you become yours...       4.5   \n",
       "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  \n",
       "0     3.5         3.0          3.0      4.0          3.0  \n",
       "1     2.5         3.0          2.0      2.0          2.5  \n",
       "2     3.5         3.0          3.0      3.0          2.5  \n",
       "3     4.5         4.5          4.5      4.0          5.0  \n",
       "4     3.0         3.0          3.0      2.5          2.5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(f\"{CFG.INPUT}/train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd1063a-7a4a-4d76-a294-79ea60183452",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = MultilabelStratifiedKFold(n_splits=CFG.N_FOLD, shuffle=True, random_state=CFG.SEED)\n",
    "for n, (train_index, valid_index) in enumerate(cv.split(train_df, train_df[CFG.TARGETS])):\n",
    "    train_df.loc[valid_index, 'fold'] = int(n)\n",
    "train_df['fold'] = train_df['fold'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0405b2f-1618-4f1c-8ef1-0dae6cd3c444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\n",
    "TOKENIZER.save_pretrained(CFG.OUTPUT+'tokenizer/')\n",
    "CFG.TOKENIZER = TOKENIZER\n",
    "del TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "664f9ec7-f0e3-4afd-af42-e839b22013b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FB3Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = df['full_text'].values\n",
    "        self.labels = df[CFG.TARGETS].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = CFG.TOKENIZER.encode_plus(\n",
    "            self.texts[item], \n",
    "            return_tensors=None, \n",
    "            add_special_tokens=True, \n",
    "            max_length=CFG.MAX_LEN,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long) \n",
    "        label = torch.tensor(self.labels[item], dtype=torch.float)\n",
    "        return inputs, label\n",
    "    \n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89115c61-98a1-49bc-8f65-813c256a5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(CFG.MODEL_NAME, output_hidden_states=True)\n",
    "        self.config.hidden_dropout = 0.\n",
    "        self.config.hidden_dropout_prob = 0.\n",
    "        self.config.attention_dropout = 0.\n",
    "        self.config.attention_probs_dropout_prob = 0.\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(CFG.MODEL_NAME, config=self.config)\n",
    "\n",
    "        if CFG.GRAD_CHECKPOINT:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        \n",
    "        self.layer_pool = None\n",
    "        #self.pool = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 6)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        \n",
    "        #last_hidden_states = outputs[0] \n",
    "        #feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "        \n",
    "        #last_hidden_states = outputs[0]\n",
    "        #att_weights = self.pool(last_hidden_states)\n",
    "        #feature =  torch.sum(att_weights * last_hidden_states, dim=1)\n",
    "        \n",
    "        all_hidden_states = torch.stack(outputs[1])\n",
    "        weighted_pooling_embeddings = self.layer_pool(all_hidden_states)\n",
    "        #feature = weighted_pooling_embeddings[:, 0]\n",
    "        feature = self.pool(weighted_pooling_embeddings, inputs['attention_mask'])\n",
    "        \n",
    "        output = self.fc(feature)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n",
    "            )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6878b9d0-7f2c-4822-941d-099bd63e58fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        adv_param=\"weight\",\n",
    "        adv_lr=1,\n",
    "        adv_eps=0.2,\n",
    "        start_epoch=0,\n",
    "        adv_step=1,\n",
    "        scaler=None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.start_epoch = start_epoch\n",
    "        self.adv_step = adv_step\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def attack_backward(self, inputs, labels, epoch):\n",
    "        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n",
    "            return None\n",
    "\n",
    "        self._save() \n",
    "        for i in range(self.adv_step):\n",
    "            self._attack_step() \n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_preds = self.model(inputs)\n",
    "                adv_loss = self.criterion(y_preds, labels)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(adv_loss).backward()\n",
    "            \n",
    "        self._restore()\n",
    "\n",
    "    def _attack_step(self):\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
    "                    )\n",
    "                # param.data.clamp_(*self.backup_eps[name])\n",
    "\n",
    "    def _save(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(self,):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d4e3904-4e48-4a72-886b-ac1a679605cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/143764\n",
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon=1., emb_name='word_embeddings'):\n",
    "        \"\"\"\n",
    "        敵対的な摂動を求め、現在のembedding layerに摂動を加える\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "                    \n",
    "    def restore(self, emb_name='word_embeddings'):\n",
    "        \"\"\"\n",
    "        敵対的な摂動を求める際に変更してしまったembedding layerのパラメータについて\n",
    "        元のパラメータを代入する\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "            self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07edfad5-2063-4573-bc4a-9b34586680b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def MCRMSE(y_trues, y_preds):\n",
    "    scores = []\n",
    "    idxes = y_trues.shape[1]\n",
    "    for i in range(idxes):\n",
    "        y_true = y_trues[:,i]\n",
    "        y_pred = y_preds[:,i]\n",
    "        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n",
    "        scores.append(score)\n",
    "    mcrmse_score = np.mean(scores)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_score(y_trues, y_preds):\n",
    "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6a1806b-ce21-4962-a2fc-452669f33fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "         'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cca8634-792a-4735-b866-3f7cf5496f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "\n",
    "class PriorWD(Optimizer):\n",
    "    def __init__(self, optim, use_prior_wd=False, exclude_last_group=True):\n",
    "        super(PriorWD, self).__init__(optim.param_groups, optim.defaults)\n",
    "        self.param_groups = optim.param_groups\n",
    "        self.optim = optim\n",
    "        self.use_prior_wd = use_prior_wd\n",
    "        self.exclude_last_group = exclude_last_group\n",
    "        self.weight_decay_by_group = []\n",
    "        for i, group in enumerate(self.param_groups):\n",
    "            self.weight_decay_by_group.append(group[\"weight_decay\"])\n",
    "            group[\"weight_decay\"] = 0\n",
    "\n",
    "        self.prior_params = {}\n",
    "        for i, group in enumerate(self.param_groups):\n",
    "            for p in group[\"params\"]:\n",
    "                self.prior_params[id(p)] = p.detach().clone()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        if self.use_prior_wd:\n",
    "            for i, group in enumerate(self.param_groups):\n",
    "                for p in group[\"params\"]:\n",
    "                    if self.exclude_last_group and i == len(self.param_groups):\n",
    "                        p.data.add_(-group[\"lr\"] * self.weight_decay_by_group[i], p.data)\n",
    "                    else:\n",
    "                        p.data.add_(\n",
    "                            -group[\"lr\"] * self.weight_decay_by_group[i], p.data - self.prior_params[id(p)],\n",
    "                        )\n",
    "        loss = self.optim.step(closure)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def compute_distance_to_prior(self, param):\n",
    "        assert id(param) in self.prior_params, \"parameter not in PriorWD optimizer\"\n",
    "        return (param.data - self.prior_params[id(param)]).pow(2).sum().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4211037e-2c1e-46cd-8d6d-346fd702f09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(epoch, train_loader, model, awp, fgm, criterion, optimizer, scheduler):\n",
    "    model.train()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=is_gpu):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "            \n",
    "        if CFG.ACCUMLATION > 1:\n",
    "            loss = loss / CFG.ACCUMLATION\n",
    "            \n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if awp is not None:\n",
    "            awp.attack_backward(inputs, labels, epoch) \n",
    "        \n",
    "        # FGM\n",
    "        if fgm is not None and epoch < CFG.FGM_END:\n",
    "            fgm.attack(epsilon=CFG.FGM_EPS, emb_name='word_embeddings')\n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_preds = model(inputs)\n",
    "                loss_adv = criterion(y_preds, labels)\n",
    "            #optimizer.zero_grad()\n",
    "            scaler.scale(loss_adv).backward()\n",
    "            fgm.restore()\n",
    "        #\"\"\"\n",
    "\n",
    "        \n",
    "        if (step + 1) % CFG.ACCUMLATION == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.GRAD_NORM)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            scheduler.step()\n",
    "                \n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "            \n",
    "        if CFG.ACCUMLATION > 1:\n",
    "            loss = loss / CFG.ACCUMLATION\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "255c6294-193a-4e62-a774-1ba45429fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_dfs = []\n",
    "for fold in range(CFG.N_FOLD):\n",
    "    _df = pd.read_csv(f\"./fb3-make-pseudo-label-4th/pseudo_v3_f{fold}.csv\")\n",
    "    _df = _df.rename(columns = {\"text\":\"full_text\"}).drop([\"ARI\", \"predicted_grade\", \"tokenize_length\"], axis=1)\n",
    "    _df[\"fold\"] = fold\n",
    "    _df[\"text_id\"] = [f\"pseudo_{i}\" for i in range(len(_df))]\n",
    "    pseudo_dfs.append(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ee9faee-84e9-4d85-a2b4-e553783c7f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(fold, seed):    \n",
    "    valid_df = train_df.query(f\"fold=={fold}\")\n",
    "    valid_labels = valid_df[CFG.TARGETS].values\n",
    "    train_dataset = FB3Dataset(train_df.query(f\"fold!={fold}\"))\n",
    "    #train_dataset = FB3Dataset(pseudo_dfs[fold])\n",
    "    #train_dataset = FB3Dataset(pd.concat([train_df.query(f\"fold!={fold}\"), pseudo_dfs[fold]], axis=0).reset_index(drop=True))\n",
    "    valid_dataset = FB3Dataset(valid_df)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.BS,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.N_WORKER, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.BS,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.N_WORKER, pin_memory=True, drop_last=False)\n",
    "\n",
    "    model = CustomModel()\n",
    "    torch.save(model.config, CFG.OUTPUT + 'config.pth')\n",
    "    \n",
    "    path = f\"pseudo_base/microsoft-deberta-v3-base_seed2_fold{fold}_best.pth\"   \n",
    "    #path = f\"pseudo/microsoft-deberta-v3-large_seed0_fold{fold}_best.pth\"   \n",
    "    state = torch.load(path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    \n",
    "    # Att poolig\n",
    "    #model.pool = nn.Sequential(\n",
    "    #        nn.Linear(model.config.hidden_size, model.config.hidden_size),\n",
    "    #        nn.Tanh(),\n",
    "    #        nn.Linear(model.config.hidden_size, 1),\n",
    "    #        nn.Softmax(dim=1),\n",
    "    #)\n",
    "    \n",
    "    # WeightedLayerPooling\n",
    "    layer_start = 12\n",
    "    model.layer_pool = WeightedLayerPooling(\n",
    "    　　   model.config.num_hidden_layers, \n",
    "        layer_start=layer_start, layer_weights=None\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.ENCODER_LR, \n",
    "                                                decoder_lr=CFG.DECODER_LR,\n",
    "                                                weight_decay=CFG.WEIGHT_DECAY)\n",
    "\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.ENCODER_LR, eps=CFG.EPS, betas=CFG.BETAS, correct_bias=True)\n",
    "    optimizer = PriorWD(optimizer, use_prior_wd=True)\n",
    "    \n",
    "    num_train_steps = int(len(train_dataset) / CFG.BS * CFG.N_EPOCH)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=CFG.N_WARMUP, num_training_steps=num_train_steps, num_cycles=CFG.N_CYCLES\n",
    "    )\n",
    "    \n",
    "    criterion = nn.SmoothL1Loss(reduction='mean', beta=1.0)\n",
    "    awp = AWP(model,\n",
    "              optimizer,\n",
    "              criterion,\n",
    "              adv_lr=CFG.ADV_LR,\n",
    "              adv_eps=CFG.ADV_EPS,\n",
    "              start_epoch=CFG.ADV_START,\n",
    "              scaler=scaler\n",
    "    )\n",
    "    fgm = FGM(model)\n",
    "\n",
    "    best_score = float(\"inf\")\n",
    "    best_predictions = None\n",
    "    for epoch in range(CFG.N_EPOCH):\n",
    "        avg_loss = train_fn(epoch, train_loader, model, awp, fgm, criterion, optimizer, scheduler)\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion)\n",
    "        score, _ = get_score(valid_labels, predictions)\n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            best_predictions = predictions\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                         f\"{CFG.OUTPUT}/{CFG.MODEL_NAME.replace('/', '-')}_seed{seed}_fold{fold}_best.pth\")\n",
    "        print(f\"[Fold-{fold}] epoch-{epoch}: score={score}\")\n",
    "        pd.DataFrame([score], columns=[\"score\"]).to_csv(f\"{CFG.OUTPUT}/fold{fold}_epoch{epoch}.csv\", index=None)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b814560b-f576-42a6-9685-bdf6cf6f2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(seed):\n",
    "    seed_everything(seed)\n",
    "    scores = []\n",
    "    for fold in range(CFG.N_FOLD):\n",
    "        if fold in CFG.SKIP_FOLDS:\n",
    "            continue\n",
    "        score = train_loop(fold, seed)\n",
    "        scores.append(score)\n",
    "    print(scores)\n",
    "    print(sum(scores) / CFG.N_FOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c7488d-de0c-447a-a41f-4121cd02fa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe655712974a4fa381d6f7545058ab6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold-0] epoch-0: score=0.456422611186763\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3b32c20c1444079e7e0d2411e8e17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold-0] epoch-1: score=0.4519258736990948\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697536a172d94476b5f8dc3448a7780d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold-0] epoch-2: score=0.4505184539010803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8450aa25b26b4506bff4681d55049b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(CFG.LOCAL_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5388be1e-7a67-4260-92b7-6b360315074a",
   "metadata": {},
   "source": [
    "# Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dc57a6-4269-491e-ac38-7a5e5a50f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets init -p {CFG.OUTPUT}\n",
    "\n",
    "with open(f\"{CFG.OUTPUT}/dataset-metadata.json\", \"r\") as f:\n",
    "    d = json.load(f)\n",
    "    \n",
    "t = f\"FB3 {CFG.EXP} output\"\n",
    "d['title'] = t\n",
    "d['id'] = \"takamichitoda/\"+\"-\".join(t.split())\n",
    "\n",
    "with open(f\"{CFG.OUTPUT}/dataset-metadata.json\", \"w\") as f:\n",
    "    json.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07748a1-a96a-4e47-949a-f6e737955305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle datasets create -p {CFG.OUTPUT}\n",
    "!kaggle datasets version -m \"test\" -p {CFG.OUTPUT}/\n",
    "\n",
    "!kaggle datasets list -m --sort-by \"updated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b047d-80a8-4c10-8c93-0c212b71142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9868603e-46a7-43e7-93e4-15a5a0648625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
