{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a5d87b-d5d4-4486-a353-88284be30804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 16 23:57:46 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   58C    P0    29W /  70W |      0MiB / 15360MiB |     10%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7255a110-17df-4771-bf1d-f9a2bdb8691d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle API 1.5.12\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p .kaggle\n",
    "!cp \"./kaggle.json\" .kaggle/\n",
    "!chmod 600 .kaggle/kaggle.json\n",
    "!cp -r .kaggle /root\n",
    "\n",
    "!kaggle -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68a6b01-ae1b-4665-9595-f731568ab779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting iterative-stratification==0.1.7\n",
      "  Downloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.0.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.7.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.21.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.7) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.7) (1.0.1)\n",
      "Installing collected packages: iterative-stratification\n",
      "Successfully installed iterative-stratification-0.1.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install iterative-stratification==0.1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6281df0d-d301-4bd9-859d-08165783c6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n",
      "transformers.__version__: 4.20.1\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "is_gpu = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_gpu else 'cpu')\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=is_gpu)\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82aabb7-f424-4f63-b131-a84035f1441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    EXP = \"exp211\"\n",
    "    INPUT = \"/home/jupyter/feedback-prize-english-language-learning\"\n",
    "    OUTPUT = f\"/home/jupyter/{EXP}/\"\n",
    "    SEED = 0\n",
    "    N_FOLD = 4\n",
    "    TARGETS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "\n",
    "    MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "    #MODEL_NAME = \"funnel-transformer/small\"\n",
    "    TOKENIZER = None\n",
    "    MAX_LEN = 768  # 1428\n",
    "    GRAD_CHECKPOINT = True\n",
    "    \n",
    "    N_EPOCH = 4\n",
    "    N_WORKER = 4\n",
    "    ENCODER_LR = 2e-5\n",
    "    DECODER_LR = 1e-3\n",
    "    EPS = 1e-6\n",
    "    BETAS = (0.9, 0.999)\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    N_WARMUP = 0\n",
    "    N_CYCLES = 0.5\n",
    "    \n",
    "    BS = 8\n",
    "    ACCUMLATION = 1\n",
    "    \n",
    "    GRAD_NORM = 0.1\n",
    "    \n",
    "    ADV_EPS = 1e-4\n",
    "    ADV_LR = 1e-4\n",
    "    ADV_START = 2\n",
    "    \n",
    "    FGM_EPS = 1e-1\n",
    "    FGM_END = float(\"inf\")\n",
    "    \n",
    "    SKIP_FOLDS = [None]\n",
    "    LOCAL_SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4466da71-3fe8-47cc-99df-3638d530dde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>The best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>Small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n",
       "3  003885A45F42  The best time in life is when you become yours...       4.5   \n",
       "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  fold  \n",
       "0     3.5         3.0          3.0      4.0          3.0     2  \n",
       "1     2.5         3.0          2.0      2.0          2.5     3  \n",
       "2     3.5         3.0          3.0      3.0          2.5     0  \n",
       "3     4.5         4.5          4.5      4.0          5.0     0  \n",
       "4     3.0         3.0          3.0      2.5          2.5     2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(f\"{CFG.INPUT}/train.csv\")\n",
    "\n",
    "cv = MultilabelStratifiedKFold(n_splits=CFG.N_FOLD, shuffle=True, random_state=CFG.SEED)\n",
    "for n, (train_index, valid_index) in enumerate(cv.split(train_df, train_df[CFG.TARGETS])):\n",
    "    train_df.loc[valid_index, 'fold'] = int(n)\n",
    "train_df['fold'] = train_df['fold'].astype(int)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11f1d229-abd5-43f7-a013-3aadb6bf3d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\n",
    "\n",
    "TOKENIZER.save_pretrained(CFG.OUTPUT+'tokenizer/')\n",
    "CFG.TOKENIZER = TOKENIZER\n",
    "del TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce162c40-387b-461d-aeba-6f94dcd14b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caea23121fd04d299fa9ad2318cbe88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3911 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1428\n",
      "before: 768\n",
      "after: 768\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "lengths = []\n",
    "tk0 = tqdm(train_df['full_text'].fillna(\"\").values, total=len(train_df))\n",
    "for text in tk0:\n",
    "    length = len(CFG.TOKENIZER(text, add_special_tokens=False)['input_ids'])\n",
    "    lengths.append(length)\n",
    "new_max_l = max(lengths) + 2 # cls & sep\n",
    "print(new_max_l)\n",
    "\n",
    "print('before:', CFG.MAX_LEN)\n",
    "#CFG.MAX_LEN = new_max_l\n",
    "print('after:', CFG.MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25ce0227-e1df-4bb0-972e-cb0fdfb8e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FB3Dataset(Dataset):\n",
    "    def __init__(self, df, train=True):\n",
    "        self.texts = df['full_text'].values\n",
    "        self.labels = df[CFG.TARGETS].values\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = CFG.TOKENIZER.encode_plus(\n",
    "            self.texts[item], \n",
    "            return_tensors=None, \n",
    "            add_special_tokens=True, \n",
    "            max_length=CFG.MAX_LEN, #if self.train else 768,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long) \n",
    "        label = torch.tensor(self.labels[item], dtype=torch.float)\n",
    "        return inputs, label\n",
    "    \n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8551082c-a1f7-44be-af30-66ae3c17b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(CFG.MODEL_NAME, output_hidden_states=True)\n",
    "        self.config.hidden_dropout = 0.\n",
    "        self.config.hidden_dropout_prob = 0.\n",
    "        self.config.attention_dropout = 0.\n",
    "        self.config.attention_probs_dropout_prob = 0.\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(CFG.MODEL_NAME, config=self.config)\n",
    "\n",
    "        if CFG.GRAD_CHECKPOINT:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        \n",
    "        # WeightedLayerPooling\n",
    "        layer_start = 12\n",
    "        self.layer_pool = WeightedLayerPooling(\n",
    "            self.config.num_hidden_layers, \n",
    "            layer_start=layer_start, layer_weights=None\n",
    "        )\n",
    "        self.pool = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 6)\n",
    "        \n",
    "        # reinit_layers\n",
    "        reinit_layer = 1\n",
    "        if reinit_layer > 0:\n",
    "            for layer in self.model.encoder.layer[-reinit_layer:]:\n",
    "                for module in layer.modules():\n",
    "                    if isinstance(module, nn.Linear):\n",
    "                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "                        if module.bias is not None:\n",
    "                            module.bias.data.zero_()\n",
    "                    elif isinstance(module, nn.Embedding):\n",
    "                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "                        if module.padding_idx is not None:\n",
    "                            module.weight.data[module.padding_idx].zero_()\n",
    "                    elif isinstance(module, nn.LayerNorm):\n",
    "                        module.bias.data.zero_()\n",
    "                        module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        \n",
    "        #pooling_embeddings = outputs[0]\n",
    "        \n",
    "        all_hidden_states = torch.stack(outputs[1])\n",
    "        pooling_embeddings = self.layer_pool(all_hidden_states)\n",
    "        \n",
    "        feature = self.pool(pooling_embeddings, inputs['attention_mask'])\n",
    "        output = self.fc(feature)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n",
    "            )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59d7bf35-f97a-4b48-bc59-2dcab6d59a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        adv_param=\"weight\",\n",
    "        adv_lr=1,\n",
    "        adv_eps=0.2,\n",
    "        start_epoch=0,\n",
    "        adv_step=1,\n",
    "        scaler=None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.start_epoch = start_epoch\n",
    "        self.adv_step = adv_step\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def attack_backward(self, inputs, labels, epoch):\n",
    "        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n",
    "            return None\n",
    "\n",
    "        self._save() \n",
    "        for i in range(self.adv_step):\n",
    "            self._attack_step() \n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_preds = self.model(inputs)\n",
    "                adv_loss = self.criterion(y_preds, labels)\n",
    "                \n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(adv_loss).backward()\n",
    "            \n",
    "        self._restore()\n",
    "\n",
    "    def _attack_step(self):\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
    "                    )\n",
    "                # param.data.clamp_(*self.backup_eps[name])\n",
    "\n",
    "    def _save(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(self,):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51fcc4ad-9cbb-43ea-8084-1033a116d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/143764\n",
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon=1., emb_name='word_embeddings'):\n",
    "        \"\"\"\n",
    "        敵対的な摂動を求め、現在のembedding layerに摂動を加える\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "                    \n",
    "    def restore(self, emb_name='word_embeddings'):\n",
    "        \"\"\"\n",
    "        敵対的な摂動を求める際に変更してしまったembedding layerのパラメータについて\n",
    "        元のパラメータを代入する\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "            self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca222f10-6a3c-4b5a-8724-156a68715edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def MCRMSE(y_trues, y_preds):\n",
    "    scores = []\n",
    "    idxes = y_trues.shape[1]\n",
    "    for i in range(idxes):\n",
    "        y_true = y_trues[:,i]\n",
    "        y_pred = y_preds[:,i]\n",
    "        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n",
    "        scores.append(score)\n",
    "    mcrmse_score = np.mean(scores)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_score(y_trues, y_preds):\n",
    "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afca21d3-dc6f-4456-bc18-d1c968a2b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
    "    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
    "    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': weight_decay, 'lr': encoder_lr/4.0},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': weight_decay, 'lr': encoder_lr/2.0},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': weight_decay, 'lr': encoder_lr},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': encoder_lr/4.0},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': encoder_lr/2.0},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': encoder_lr},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n], 'lr': decoder_lr, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ff45896-4223-4c49-9b67-82ab39e5f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "\n",
    "class PriorWD(Optimizer):\n",
    "    def __init__(self, optim, use_prior_wd=False, exclude_last_group=True):\n",
    "        super(PriorWD, self).__init__(optim.param_groups, optim.defaults)\n",
    "        self.param_groups = optim.param_groups\n",
    "        self.optim = optim\n",
    "        self.use_prior_wd = use_prior_wd\n",
    "        self.exclude_last_group = exclude_last_group\n",
    "        self.weight_decay_by_group = []\n",
    "        for i, group in enumerate(self.param_groups):\n",
    "            self.weight_decay_by_group.append(group[\"weight_decay\"])\n",
    "            group[\"weight_decay\"] = 0\n",
    "\n",
    "        self.prior_params = {}\n",
    "        for i, group in enumerate(self.param_groups):\n",
    "            for p in group[\"params\"]:\n",
    "                self.prior_params[id(p)] = p.detach().clone()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        if self.use_prior_wd:\n",
    "            for i, group in enumerate(self.param_groups):\n",
    "                for p in group[\"params\"]:\n",
    "                    if self.exclude_last_group and i == len(self.param_groups):\n",
    "                        p.data.add_(-group[\"lr\"] * self.weight_decay_by_group[i], p.data)\n",
    "                    else:\n",
    "                        p.data.add_(\n",
    "                            -group[\"lr\"] * self.weight_decay_by_group[i], p.data - self.prior_params[id(p)],\n",
    "                        )\n",
    "        loss = self.optim.step(closure)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def compute_distance_to_prior(self, param):\n",
    "        assert id(param) in self.prior_params, \"parameter not in PriorWD optimizer\"\n",
    "        return (param.data - self.prior_params[id(param)]).pow(2).sum().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b4ad0d4-f4d0-4590-b3b1-bcf394d04435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(epoch, train_loader, model, awp, fgm, criterion, optimizer, scheduler):\n",
    "    model.train()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=is_gpu):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "            \n",
    "        if CFG.ACCUMLATION > 1:\n",
    "            loss = loss / CFG.ACCUMLATION\n",
    "            \n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        awp.attack_backward(inputs, labels, epoch) \n",
    "        \n",
    "        # FGM\n",
    "        if epoch < CFG.FGM_END:\n",
    "            fgm.attack(epsilon=CFG.FGM_EPS, emb_name='word_embeddings')\n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_preds = model(inputs)\n",
    "                loss_adv = criterion(y_preds, labels)\n",
    "            #optimizer.zero_grad()\n",
    "            scaler.scale(loss_adv).backward()\n",
    "            fgm.restore()          \n",
    "        \n",
    "        scaler.unscale_(optimizer)\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.GRAD_NORM)\n",
    "        \n",
    "        if (step + 1) % CFG.ACCUMLATION == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            scheduler.step()\n",
    "                \n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "            \n",
    "        if CFG.ACCUMLATION > 1:\n",
    "            loss = loss / CFG.ACCUMLATION\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64e3042a-81ec-4d4f-a1f9-be2731dd6c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss(y_pred, y_true):\n",
    "    mask = nn.L1Loss(reduction='none')(y_pred, y_true) < 1.5\n",
    "    loss = nn.MSELoss(reduction='none')(y_pred, y_true)\n",
    "    loss = torch.mean(loss * mask.float())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8945c2bc-2a50-469b-a6b6-36b0d731c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(fold, seed):\n",
    "    valid_df = train_df.query(f\"fold=={fold}\")\n",
    "    valid_labels = valid_df[CFG.TARGETS].values\n",
    "    train_dataset = FB3Dataset(train_df.query(f\"fold!={fold}\"))\n",
    "    valid_dataset = FB3Dataset(valid_df)\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.BS,\n",
    "                              shuffle=True,\n",
    "                              #collate_fn=DataCollatorWithPadding(tokenizer=CFG.TOKENIZER, padding='longest'),\n",
    "                              num_workers=CFG.N_WORKER, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.BS,\n",
    "                              shuffle=False,\n",
    "                              #collate_fn=DataCollatorWithPadding(tokenizer=CFG.TOKENIZER, padding='longest'),\n",
    "                              num_workers=CFG.N_WORKER, pin_memory=True, drop_last=False)\n",
    "\n",
    "    model = CustomModel()\n",
    "    torch.save(model.config, CFG.OUTPUT + 'config.pth')\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.ENCODER_LR, \n",
    "                                                decoder_lr=CFG.DECODER_LR,\n",
    "                                                weight_decay=CFG.WEIGHT_DECAY)\n",
    "\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.ENCODER_LR, eps=CFG.EPS, betas=CFG.BETAS, correct_bias=True)\n",
    "    optimizer = PriorWD(optimizer, use_prior_wd=True)\n",
    "    \n",
    "    num_train_steps = int(len(train_dataset) / CFG.BS * CFG.N_EPOCH)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=CFG.N_WARMUP, num_training_steps=num_train_steps, num_cycles=CFG.N_CYCLES\n",
    "    )\n",
    "\n",
    "    criterion = nn.SmoothL1Loss(reduction='mean', beta=1.0)\n",
    "    #criterion = torch.nn.HuberLoss(reduction='mean', delta=1.0)\n",
    "    #criterion = svm_loss\n",
    "    \n",
    "    awp = AWP(model,\n",
    "              optimizer,\n",
    "              criterion,\n",
    "              adv_lr=CFG.ADV_LR,\n",
    "              adv_eps=CFG.ADV_EPS,\n",
    "              start_epoch=CFG.ADV_START,\n",
    "              scaler=scaler\n",
    "    )\n",
    "    fgm = FGM(model)\n",
    "\n",
    "    best_score = float(\"inf\")\n",
    "    best_predictions = None\n",
    "    for epoch in range(CFG.N_EPOCH):\n",
    "        avg_loss = train_fn(epoch, train_loader, model, awp, fgm, criterion, optimizer, scheduler)\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion)\n",
    "\n",
    "        score, _ = get_score(valid_labels, predictions)\n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            best_predictions = predictions\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                         f\"{CFG.OUTPUT}/{CFG.MODEL_NAME.replace('/', '-')}_seed{seed}_fold{fold}_best.pth\")\n",
    "        print(f\"[Fold-{fold}] epoch-{epoch}: score={score}\")\n",
    "        \n",
    "        pd.DataFrame([score], columns=[\"score\"]).to_csv(f\"{CFG.OUTPUT}/fold{fold}_epoch{epoch}.csv\", index=None)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd069237-e7f4-4726-9ced-dd2fb370d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(seed):\n",
    "    scores = []\n",
    "    for fold in range(CFG.N_FOLD):\n",
    "        if fold in CFG.SKIP_FOLDS:\n",
    "            continue\n",
    "        seed_everything(seed)\n",
    "        score = train_loop(fold, seed)\n",
    "        scores.append(score)\n",
    "    print(scores)\n",
    "    print(sum(scores) / CFG.N_FOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8395ef90-22f1-4253-9dcb-dd2a9d968e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32562b6e87894868a2b916ad6beca714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold-0] epoch-0: score=0.550643943480246\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09cbf1ddd0964c4193dea5d75b09f804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold-0] epoch-1: score=0.4686704473507413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea1bacdea3647ff9570016b72aaf2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold-0] epoch-2: score=0.4565123876796777\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40d56d679764f2d834af9f73141be3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold-0] epoch-3: score=0.4536960458522694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5568661887d246638882484145a042e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_432/1647284700.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOCAL_SEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_432/4241167340.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mseed_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_432/2591672002.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(fold, seed)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mbest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mawp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mavg_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_432/360039040.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(epoch, train_loader, model, awp, fgm, criterion, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mawp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(CFG.LOCAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5223d955-def9-40cd-be2d-aa91aa0c3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "!kaggle datasets init -p {CFG.OUTPUT}\n",
    "\n",
    "with open(f\"{CFG.OUTPUT}/dataset-metadata.json\", \"r\") as f:\n",
    "    d = json.load(f)\n",
    "    \n",
    "t = f\"FB3 {CFG.EXP} output\"\n",
    "d['title'] = t\n",
    "d['id'] = \"takamichitoda/\"+\"-\".join(t.split())\n",
    "\n",
    "with open(f\"{CFG.OUTPUT}/dataset-metadata.json\", \"w\") as f:\n",
    "    json.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b17b0-0218-410c-bf47-bf9e0d21abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets create -p {CFG.OUTPUT}\n",
    "#!kaggle datasets version -m \"test\" -p {CFG.OUTPUT}/\n",
    "\n",
    "!kaggle datasets list -m --sort-by \"updated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc99225-e670-483d-891a-e309ba2e909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ac2c0f-161f-49d1-8545-44533fdde191",
   "metadata": {},
   "source": [
    "# add exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3654a4-6756-4266-97d3-3a4c6b4ad021",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59011cb7-e41a-4702-bbc3-4b8ae9e59847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
