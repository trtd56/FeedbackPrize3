{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1ek5aQrgIF8ntI-v1nQ_i03VJoJuvu-CZ","timestamp":1666828408925},{"file_id":"1NmGK8VvoIirYigmaOEFr-e3n7qkiKxqp","timestamp":1665453075831}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"VU9pMLG-w8f1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gSsct57h0otA"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install -q kaggle\n","!mkdir -p .kaggle\n","!cp \"./drive/MyDrive/Study/config/kaggle.json\" .kaggle/\n","!chmod 600 .kaggle/kaggle.json\n","!mv .kaggle /root"],"metadata":{"id":"B9AxpyLOczUS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# competition data\n","!kaggle competitions download -c feedback-prize-english-language-learning\n","!unzip feedback-prize-english-language-learning.zip\n","!rm -rf feedback-prize-english-language-learning.zip\n","!mkdir -p competition_data\n","!mv sample_submission.csv test.csv train.csv competition_data/\n","!echo \"### competition data ###\"\n","!ls /content/competition_data/\n","!echo\n","\n","# pseudo labels\n","!mkdir -p pseudo_labels\n","!kaggle kernels output takamichitoda/fb3-make-pseudo-label-4th -p /content/pseudo_labels\n","!echo \"### pseudo labels ###\"\n","!ls /content/pseudo_labels\n","!echo\n","\n","# prediction 3rd\n","#!mkdir -p prediction_3rd\n","#!kaggle kernels output takamichitoda/fb3-oof-feature-v2-0 -p prediction_3rd/\n","\n","# train models 3rd\n","!mkdir -p train_models_3rd\n","!kaggle kernels output takamichitoda/fb3-awp-pseudo-3nd -p /content/train_models_3rd\n","!cd /content/train_models_3rd && rm -rf microsoft-deberta-v3-base_seed0_fold1_best.pth microsoft-deberta-v3-base_seed0_fold3_best.pth\n","!kaggle kernels output takamichitoda/fb3-awp-pseudo-3nd-fold-1-only -p /content/train_models_3rd\n","!kaggle datasets download -d takamichitoda/fb3-deberta-v3-pseudo-3rd\n","!unzip fb3-deberta-v3-pseudo-3rd.zip\n","!rm -rf fb3-deberta-v3-pseudo-3rd.zip\n","!mv microsoft-deberta-v3-base_seed0_fold3_best.pth train_models_3rd/\n","!echo \"### train models 3rd ###\"\n","!ls /content/train_models_3rd"],"metadata":{"id":"fjMzivIjw4rF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install iterative-stratification==0.1.7\n","!pip install transformers\n","!pip install sentencepiece"],"metadata":{"id":"pYwQp1r0d-sj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## メイン処理"],"metadata":{"id":"vmnEYNHixU8G"}},{"cell_type":"code","source":["import gc\n","import json\n","import os\n","import random\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import numpy as np\n","import pandas as pd\n","\n","from tqdm.auto import tqdm\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","from sklearn.metrics import mean_squared_error\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","#from torch.optim import AdamW\n","from transformers import AdamW\n","from torch.utils.checkpoint import checkpoint\n","\n","import transformers\n","from transformers import AutoTokenizer, AutoModel, AutoConfig\n","from transformers import get_cosine_schedule_with_warmup\n","\n","is_gpu = torch.cuda.is_available()\n","device = torch.device('cuda' if is_gpu else 'cpu')\n","scaler = torch.cuda.amp.GradScaler(enabled=is_gpu)\n","\n","%env TOKENIZERS_PARALLELISM=true\n","print(device)\n","print(f\"transformers.__version__: {transformers.__version__}\")"],"metadata":{"id":"5Twer-e_dGmo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CFG:\n","    EXP_NUM = 9\n","    MEMO = \"round\"\n","\n","    INPUT = \"/content/competition_data\"\n","    OUTPUT = f\"/content/output/distribution_pos_mask/\"\n","    SEED = 0\n","    N_FOLD = 4\n","    TARGETS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n","    \n","    MODEL_NAME = \"microsoft/deberta-v3-xsmall\"\n","    TOKENIZER = None\n","    MAX_LEN = 512\n","    GRAD_CHECKPOINT = False\n","    \n","    N_EPOCH = 16\n","    N_WORKER = 2\n","    ENCODER_LR = 1e-4\n","    DECODER_LR = 5e-3\n","\n","    EPS = 1e-6\n","    BETAS = (0.9, 0.999)\n","    WEIGHT_DECAY = 0.1\n","    N_WARMUP = 0\n","    N_CYCLES = 0.5\n","    \n","    BS = 64 #// 2\n","    ACCUMLATION = 1\n","    \n","    GRAD_NORM = 0.1\n","\n","    SVR_EPS = 0.1\n","    #SVR_C = 0.01\n","\n","    MASK_RATIO = 0.1\n","    \n","    SKIP_FOLDS = [None]\n","    LOCAL_SEED = 0\n","\n","!mkdir -p {CFG.OUTPUT}\n","!kaggle datasets init -p {CFG.OUTPUT}\n","\n","with open(f'{CFG.OUTPUT}/dataset-metadata.json', 'r') as f:\n","    d = json.load(f)\n","t = f'FB3 distribution roundhead'\n","d['title'] = t\n","d['id'] = f'takamichitoda/'+ t.replace(' ', '-')\n","print(d)\n","with open(f'{CFG.OUTPUT}/dataset-metadata.json', 'w') as f:\n","    json.dump(d, f)\n","del d\n","\n","s = {k:v for k, v in vars(CFG).items() if \"__\" != k[:2]}\n","with open(f'{CFG.OUTPUT}/setting.json', 'w') as f:\n","    json.dump(s, f)\n","del s\n","\n","!rm -rf {CFG.OUTPUT}/tokenizer*\n","TOKENIZER = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\n","TOKENIZER.save_pretrained(CFG.OUTPUT+'tokenizer/')\n","CFG.TOKENIZER = TOKENIZER\n","del TOKENIZER\n","!zip -r tokenizer.zip {CFG.OUTPUT}/tokenizer\n","!mv tokenizer.zip {CFG.OUTPUT}"],"metadata":{"id":"-Gq8LiUmdbwS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = pd.read_csv(f\"{CFG.INPUT}/train.csv\")\n","\n","cv = MultilabelStratifiedKFold(n_splits=CFG.N_FOLD, shuffle=True, random_state=CFG.SEED)\n","for n, (train_index, valid_index) in enumerate(cv.split(train_df, train_df[CFG.TARGETS])):\n","    train_df.loc[valid_index, 'fold'] = int(n)\n","train_df['fold'] = train_df['fold'].astype(int)\n","\n","train_df.head()"],"metadata":{"id":"O547xsDoedqy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MASK_ID = 128000\n","def masking(inputs):\n","    is_special_token = (np.array(inputs[\"input_ids\"]) == 1).astype(int) + (np.array(inputs[\"input_ids\"]) == 2).astype(int)\n","    mask = np.array([random.random() < CFG.MASK_RATIO for _ in range(CFG.MAX_LEN)])\n","    mask = mask * np.array(inputs[\"attention_mask\"]) * (1 - is_special_token)\n","    masking_ids = np.where(mask != 0, MASK_ID, inputs[\"input_ids\"])\n","    return masking_ids"],"metadata":{"id":"U0i4L-CFib1x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FB3Dataset(Dataset):\n","    def __init__(self, df, train=False):\n","        self.texts = df['full_text'].values\n","        self.labels = df[CFG.TARGETS].values\n","        self.train = train\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, item):\n","        if self.train:\n","            \"\"\"\n","            inputs = CFG.TOKENIZER.encode_plus(\n","                self.texts[item], \n","                return_tensors=None, \n","                add_special_tokens = False,\n","            )\n","\n","            if len(inputs['input_ids']) > CFG.MAX_LEN:\n","                l = CFG.MAX_LEN - 2\n","                start_i = random.sample(list(range(len(inputs['input_ids']) - l)), 1)\n","                start_i = start_i[0]\n","                inputs['input_ids'] = [1] + inputs['input_ids'][start_i:start_i+l] + [2]\n","                inputs['attention_mask'] = [1] + inputs['attention_mask'][start_i:start_i+l] + [1]\n","                inputs['token_type_ids'] = [0] + inputs['token_type_ids'][start_i:start_i+l] + [0]\n","\n","            pad = [0] * (CFG.MAX_LEN - len(inputs['input_ids']))\n","            inputs['input_ids'] += pad\n","            inputs['attention_mask'] += pad\n","            inputs['token_type_ids'] += pad\n","            \"\"\"\n","            inputs = CFG.TOKENIZER.encode_plus(\n","                self.texts[item], \n","                return_tensors=None, \n","                add_special_tokens=True, \n","                max_length=CFG.MAX_LEN,\n","                pad_to_max_length=True,\n","                truncation=True\n","            )\n","            # masking\n","            inputs[\"input_ids\"] = masking(inputs)\n","        else:\n","            inputs = CFG.TOKENIZER.encode_plus(\n","                self.texts[item], \n","                return_tensors=None, \n","                add_special_tokens=True, \n","                max_length=CFG.MAX_LEN,\n","                pad_to_max_length=True,\n","                truncation=True\n","            )\n","\n","        for k, v in inputs.items():\n","            inputs[k] = torch.tensor(v, dtype=torch.long) \n","        label = torch.tensor(self.labels[item], dtype=torch.float)\n","        return inputs, label\n","    \n","\n","def collate(inputs):\n","    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n","    for k, v in inputs.items():\n","        inputs[k] = inputs[k][:,:mask_len]\n","    return inputs"],"metadata":{"id":"iXjowusfehsq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class WeightedLayerPooling(nn.Module):\n","    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n","        super(WeightedLayerPooling, self).__init__()\n","        self.layer_start = layer_start\n","        self.num_hidden_layers = num_hidden_layers\n","        self.layer_weights = layer_weights if layer_weights is not None \\\n","            else nn.Parameter(\n","                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n","            )\n","\n","    def forward(self, all_hidden_states):\n","        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n","        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n","        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n","        return weighted_average\n","\n","class MeanPooling(nn.Module):\n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        mean_embeddings = sum_embeddings / sum_mask\n","        return mean_embeddings\n","\n","class AttentionPool(nn.Module):\n","    def __init__(self, in_dim):\n","        super().__init__()\n","        self.attention = nn.Sequential(\n","        nn.Linear(in_dim, in_dim),\n","        nn.LayerNorm(in_dim),\n","        nn.GELU(),\n","        nn.Linear(in_dim, 1),\n","        )\n","\n","    def forward(self, x, mask):\n","        w = self.attention(x).float() #\n","        w[mask==0]=float('-inf')\n","        w = torch.softmax(w,1)\n","        x = torch.sum(w * x, dim=1)\n","        return x\n","\n","class CustomModel(nn.Module):\n","    def __init__(self, model_name):\n","        super().__init__()\n","\n","        self.config = AutoConfig.from_pretrained(model_name, output_hidden_states=True)\n","        self.config.hidden_dropout = 0.\n","        self.config.hidden_dropout_prob = 0.\n","        self.config.attention_dropout = 0.\n","        self.config.attention_probs_dropout_prob = 0.\n","        self.config.num_hidden_layers = 4\n","\n","        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n","\n","        if CFG.GRAD_CHECKPOINT:\n","            self.model.gradient_checkpointing_enable()\n","            \n","        \n","        layer_start = self.config.num_hidden_layers\n","        self.layer_pool = WeightedLayerPooling(\n","            self.config.num_hidden_layers, \n","            layer_start=layer_start, layer_weights=None\n","        )\n","        self.pool = MeanPooling()\n","        #self.pool = AttentionPool(self.config.num_hidden_layers)\n","        #self.pool = nn.Sequential(\n","        #    nn.Linear(self.config.hidden_size, self.config.hidden_size),\n","        #    nn.Tanh(),\n","        #    nn.Linear(self.config.hidden_size, 1),\n","        #    nn.Softmax(dim=1),\n","        #)\n","        #self.fc = nn.Linear(self.config.hidden_size, 6)\n","\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(self.config.hidden_size, self.config.hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(self.config.hidden_size, 6),\n","        )\n","        self._init_weights(self.fc1)\n","        self.fc2 = nn.Sequential(\n","            nn.Linear(self.config.hidden_size, self.config.hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(self.config.hidden_size, 6),\n","        )\n","        self._init_weights(self.fc2)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(self, inputs):\n","        outputs = self.model(**inputs)\n","\n","        #last_hidden_states = outputs[0]\n","        #feature = self.pool(last_hidden_states, inputs['attention_mask'])\n","\n","        all_hidden_states = torch.stack(outputs[1])\n","        weighted_pooling_embeddings = self.layer_pool(all_hidden_states)\n","        feature = self.pool(weighted_pooling_embeddings, inputs['attention_mask'])\n","\n","        #input_mask_expanded = inputs['attention_mask'].unsqueeze(-1).expand(weighted_pooling_embeddings.size()).float()\n","        #att_weights = self.pool(weighted_pooling_embeddings * input_mask_expanded)\n","        #feature =  torch.sum(att_weights * weighted_pooling_embeddings, dim=1)\n","\n","        output1 = self.fc1(feature)\n","        output2 = self.fc2(feature)\n","\n","        return output1, output2"],"metadata":{"id":"qWyd6CohgZYC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        \n","def MCRMSE(y_trues, y_preds):\n","    scores = []\n","    idxes = y_trues.shape[1]\n","    for i in range(idxes):\n","        y_true = y_trues[:,i]\n","        y_pred = y_preds[:,i]\n","        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n","        scores.append(score)\n","    mcrmse_score = np.mean(scores)\n","    return mcrmse_score, scores\n","\n","\n","def get_score(y_trues, y_preds):\n","    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n","    return mcrmse_score, scores\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"],"metadata":{"id":"1IMrGYMJfdAO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_parameters = [\n","        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","         'lr': encoder_lr, 'weight_decay': weight_decay},\n","        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n","         'lr': encoder_lr, 'weight_decay': 0.0},\n","        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n","         'lr': decoder_lr, 'weight_decay': 0.0}\n","    ]\n","    return optimizer_parameters"],"metadata":{"id":"FugLmIjdfj1W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_fn(epoch, train_loader, student, criterion, optimizer, scheduler):\n","    student.train()\n","\n","    losses = AverageMeter()\n","    global_step = 0\n","    for step, (inputs, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n","        inputs = collate(inputs)\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","\n","        labels1 = torch.ceil(labels)\n","        labels2 = torch.floor(labels)\n","        \n","        batch_size = labels.size(0)\n","        with torch.cuda.amp.autocast(enabled=is_gpu):\n","            #y_preds = student(inputs)\n","            y_preds1, y_preds2 = student(inputs)\n","\n","            #l2 = torch.tensor(0., requires_grad=True)\n","            #for w in student.fc.parameters():\n","            #    #l1 = l1 + torch.norm(w, 1)\n","            #    l2 = l2 + torch.norm(w)**2\n","            \n","            #loss = criterion(y_preds, labels)\n","            loss1 = criterion(y_preds1, labels1)\n","            loss2 = criterion(y_preds2, labels2)\n","            loss = loss1 + loss2\n","\n","        if CFG.ACCUMLATION > 1:\n","            loss = loss / CFG.ACCUMLATION\n","            \n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        scaler.unscale_(optimizer)\n","        grad_norm = torch.nn.utils.clip_grad_norm_(student.parameters(), CFG.GRAD_NORM)\n","        \n","        if (step + 1) % CFG.ACCUMLATION == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","            global_step += 1\n","            scheduler.step()\n","                \n","    return losses.avg\n","\n","def valid_fn(valid_loader, model, criterion):\n","    losses = AverageMeter()\n","    model.eval()\n","    preds = []\n","    for step, (inputs, labels) in enumerate(valid_loader):\n","        inputs = collate(inputs)\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.no_grad():\n","            #y_preds = model(inputs)\n","            y_preds1, y_preds2 = model(inputs)\n","            y_preds = (y_preds1 + y_preds2) / 2\n","            loss = criterion(y_preds, labels)\n","            \n","        if CFG.ACCUMLATION > 1:\n","            loss = loss / CFG.ACCUMLATION\n","        losses.update(loss.item(), batch_size)\n","        preds.append(y_preds.to('cpu').numpy())\n","\n","    predictions = np.concatenate(preds)\n","    return losses.avg, predictions"],"metadata":{"id":"hedRPSQnflVs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pseudo_dfs = []\n","for fold in range(CFG.N_FOLD):\n","    _df = pd.read_csv(f\"/content/pseudo_labels/pseudo_v3_f{fold}.csv\")\n","    _df = _df.rename(columns = {\"text\":\"full_text\"}).drop([\"ARI\", \"predicted_grade\", \"tokenize_length\"], axis=1)\n","    _df[\"fold\"] = fold\n","    pseudo_dfs.append(_df[CFG.TARGETS])\n","_df[CFG.TARGETS] = sum(pseudo_dfs) / 4\n","_df[\"text_id\"] = [f\"pseudo_{i}\" for i in range(len(_df))]\n","pseudo_df = _df\n","\n","pseudo_df.head()"],"metadata":{"id":"-wSlzorjgpXN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#predict_lst = []\n","#for fold in range(CFG.N_FOLD):\n","#    p = np.load(f'/content/prediction_3rd/oof_feat_l1428_fold{fold}.npy')\n","#    predict_lst.append(p[])\n","\n","oof = np.zeros((len(train_df), 6))\n","for fold in range(CFG.N_FOLD):\n","    state = torch.load(f\"/content/train_models_3rd/microsoft-deberta-v3-base_seed0_fold{fold}_best.pth\", map_location=torch.device('cpu'))\n","    oof[train_df.query(f\"fold=={fold}\").index, :] = state['predictions']"],"metadata":{"id":"iJhyDWdI3pT3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def svm_loss(y_pred, y_true):\n","    #loss = nn.L1Loss(reduction='none')(y_pred, y_true)\n","    #loss = torch.mean(torch.clamp(loss - CFG.SVR_EPS, min=0))\n","    #mask = nn.L1Loss(reduction='none')(y_pred, y_true) < 1.5\n","    loss = nn.MSELoss(reduction='none')(y_pred, y_true)\n","    #loss = loss * mask.float()\n","    loss = torch.mean(torch.clamp(loss - CFG.SVR_EPS**2, min=0))\n","    #loss = loss + L * CFG.SVR_C\n","    return loss"],"metadata":{"id":"xjiR6T4KQlCl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.optim import Optimizer\n","\n","class PriorWD(Optimizer):\n","    def __init__(self, optim, use_prior_wd=False, exclude_last_group=True):\n","        super(PriorWD, self).__init__(optim.param_groups, optim.defaults)\n","        self.param_groups = optim.param_groups\n","        self.optim = optim\n","        self.use_prior_wd = use_prior_wd\n","        self.exclude_last_group = exclude_last_group\n","        self.weight_decay_by_group = []\n","        for i, group in enumerate(self.param_groups):\n","            self.weight_decay_by_group.append(group[\"weight_decay\"])\n","            group[\"weight_decay\"] = 0\n","\n","        self.prior_params = {}\n","        for i, group in enumerate(self.param_groups):\n","            for p in group[\"params\"]:\n","                self.prior_params[id(p)] = p.detach().clone()\n","\n","    def step(self, closure=None):\n","        if self.use_prior_wd:\n","            for i, group in enumerate(self.param_groups):\n","                for p in group[\"params\"]:\n","                    if self.exclude_last_group and i == len(self.param_groups):\n","                        p.data.add_(-group[\"lr\"] * self.weight_decay_by_group[i], p.data)\n","                    else:\n","                        p.data.add_(\n","                            -group[\"lr\"] * self.weight_decay_by_group[i], p.data - self.prior_params[id(p)],\n","                        )\n","        loss = self.optim.step(closure)\n","\n","        return loss\n","\n","    def compute_distance_to_prior(self, param):\n","        assert id(param) in self.prior_params, \"parameter not in PriorWD optimizer\"\n","        return (param.data - self.prior_params[id(param)]).pow(2).sum().sqrt()"],"metadata":{"id":"4V3FC778ltgx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_loop(fold, seed):\n","    if fold == \"ALL\":\n","        valid_df = train_df\n","    else:\n","        valid_df = train_df.query(f\"fold=={fold}\")\n","\n","    _train_df = train_df.copy()\n","    _train_df[CFG.TARGETS] = oof\n","\n","    valid_labels = valid_df[CFG.TARGETS].values\n","    if fold == \"ALL\":\n","        train_dataset = FB3Dataset(pd.concat([_train_df, pseudo_df], axis=0).reset_index(drop=True), train=True)\n","    else:\n","        train_dataset = FB3Dataset(pd.concat([_train_df.query(f\"fold!={fold}\"), pseudo_df], axis=0).reset_index(drop=True), train=True)\n","    valid_dataset = FB3Dataset(valid_df, train=False)\n","    train_loader = DataLoader(train_dataset,\n","                              batch_size=CFG.BS,\n","                              shuffle=True,\n","                              num_workers=CFG.N_WORKER, pin_memory=True, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset,\n","                              batch_size=CFG.BS,\n","                              shuffle=False,\n","                              num_workers=CFG.N_WORKER, pin_memory=True, drop_last=False)\n","\n","    student = CustomModel(CFG.MODEL_NAME)\n","    torch.save(student.config, CFG.OUTPUT + 'config.pth')\n","    student.to(device)\n","\n","    optimizer_parameters = get_optimizer_params(student,\n","                                                encoder_lr=CFG.ENCODER_LR, \n","                                                decoder_lr=CFG.DECODER_LR,\n","                                                weight_decay=CFG.WEIGHT_DECAY)\n","\n","    #optimizer = AdamW(optimizer_parameters, lr=CFG.ENCODER_LR, eps=CFG.EPS, betas=CFG.BETAS)\n","    optimizer = AdamW(optimizer_parameters, lr=CFG.ENCODER_LR, eps=CFG.EPS, betas=CFG.BETAS, correct_bias=True)\n","    optimizer = PriorWD(optimizer, use_prior_wd=True)\n","\n","    num_train_steps = int(len(train_dataset) / CFG.BS * CFG.N_EPOCH)\n","    scheduler = get_cosine_schedule_with_warmup(\n","            optimizer, num_warmup_steps=CFG.N_WARMUP, num_training_steps=num_train_steps, num_cycles=CFG.N_CYCLES\n","    )\n","    #criterion = nn.SmoothL1Loss(reduction='mean', beta=1.0)\n","    #criterion = nn.HuberLoss(reduction='mean', delta=1.0)\n","    criterion = svm_loss\n","\n","    best_score = float(\"inf\")\n","    best_predictions = None\n","    results = []\n","    for epoch in range(CFG.N_EPOCH):\n","\n","        # freeze\n","        #if epoch == 0:\n","        #    for param in student.model.embeddings.parameters():\n","        #        param.requires_grad = False\n","        #else:\n","        #    for param in student.model.embeddings.parameters():\n","        #        param.requires_grad = True\n","\n","        avg_loss = train_fn(epoch, train_loader, student, criterion, optimizer, scheduler)\n","        avg_val_loss, predictions = valid_fn(valid_loader, student, criterion)\n","        score, _ = get_score(valid_labels, predictions)\n","        \n","        if best_score > score:\n","            best_score = score\n","            best_predictions = predictions\n","            torch.save({'model': student.state_dict(),\n","                        'predictions': predictions},\n","                         f\"{CFG.OUTPUT}/{CFG.MODEL_NAME.replace('/', '-')}_seed{seed}_fold{fold}_best.pth\")\n","        print(f\"[Fold-{fold}] epoch-{epoch}: score={score}\")\n","        results.append((fold, epoch, score, best_score))\n","\n","        if fold == \"ALL\":\n","            torch.save({'model': student.state_dict(),\n","                        'predictions': predictions},\n","                        f\"{CFG.OUTPUT}/{CFG.MODEL_NAME.replace('/', '-')}_seed{seed}_fold{fold}_epoch{epoch}.pth\")\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","        \n","    return best_score, results"],"metadata":{"id":"2XPpKLddftcU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main(seed):\n","    scores = []\n","    results_lst = []\n","    for fold in range(CFG.N_FOLD):\n","        if fold in CFG.SKIP_FOLDS:\n","            continue\n","        seed_everything(seed)\n","        score, results = train_loop(fold, seed)\n","        scores.append(score)\n","        results_lst += results\n","    print(scores)\n","    print(sum(scores) / CFG.N_FOLD)\n","\n","    # all\n","    #score, results = train_loop(\"ALL\", seed)\n","    #scores.append(score)\n","    #results_lst += results\n","    pd.DataFrame(results_lst, columns=['fold',  'epoch', 'score', 'best_score']).to_csv(f\"{CFG.OUTPUT}/result.csv\", index=None)"],"metadata":{"id":"Cxoi2AWjf3-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    main(CFG.LOCAL_SEED)"],"metadata":{"id":"z7TAsq4Df6bL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!kaggle datasets create -p {CFG.OUTPUT}\n","#!kaggle datasets version -p {CFG.OUTPUT} -m {CFG.MEMO}\n","#!rm -rf {CFG.OUTPUT}"],"metadata":{"id":"DPXGe80qzWtN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"amFZKD2zCi-b"},"execution_count":null,"outputs":[]}]}