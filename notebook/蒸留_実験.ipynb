{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"VU9pMLG-w8f1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gSsct57h0otA"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install -q kaggle\n","!mkdir -p .kaggle\n","!cp \"./drive/MyDrive/Study/config/kaggle.json\" .kaggle/\n","!chmod 600 .kaggle/kaggle.json\n","!mv .kaggle /root"],"metadata":{"id":"B9AxpyLOczUS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!kaggle datasets download -d takamichitoda/fb3-deberta-v3-awp\n","!kaggle competitions download -c feedback-prize-english-language-learning\n","!kaggle kernels output takamichitoda/fb3-make-pseudo-label-2nd\n","\n","!unzip feedback-prize-english-language-learning.zip\n","!unzip fb3-deberta-v3-awp.zip\n","!rm -rf feedback-prize-english-language-learning.zip  fb3-deberta-v3-awp.zip"],"metadata":{"id":"U9swCybjdAfO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install iterative-stratification==0.1.7\n","!pip install transformers\n","!pip install sentencepiece"],"metadata":{"id":"pYwQp1r0d-sj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## メイン処理"],"metadata":{"id":"vmnEYNHixU8G"}},{"cell_type":"code","source":["import gc\n","import os\n","import random\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import numpy as np\n","import pandas as pd\n","\n","from tqdm.auto import tqdm\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","from sklearn.metrics import mean_squared_error\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from torch.optim import AdamW\n","from torch.utils.checkpoint import checkpoint\n","\n","import transformers\n","from transformers import AutoTokenizer, AutoModel, AutoConfig\n","from transformers import get_cosine_schedule_with_warmup\n","\n","\n","is_gpu = torch.cuda.is_available()\n","device = torch.device('cuda' if is_gpu else 'cpu')\n","scaler = torch.cuda.amp.GradScaler(enabled=is_gpu)\n","\n","%env TOKENIZERS_PARALLELISM=true\n","print(device)\n","print(f\"transformers.__version__: {transformers.__version__}\")"],"metadata":{"id":"5Twer-e_dGmo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CFG:\n","    INPUT = \"/content\"\n","    OUTPUT = \"/content/drive/MyDrive/Study/FB3/output\"\n","    SEED = 0\n","    N_FOLD = 4\n","    TARGETS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n","    \n","    MODEL_NAME = \"microsoft/deberta-v3-base\"\n","    TOKENIZER = None\n","    MAX_LEN = 1429 - 1\n","    GRAD_CHECKPOINT = True\n","    \n","    N_EPOCH = 4\n","    N_WORKER = 4\n","    ENCODER_LR = 2e-5\n","    DECODER_LR = 2e-5\n","    EPS = 1e-6\n","    BETAS = (0.9, 0.999)\n","    WEIGHT_DECAY = 0.01\n","    N_WARMUP = 0\n","    N_CYCLES = 0.5\n","    \n","    BS = 8\n","    ACCUMLATION = 1\n","    \n","    GRAD_NORM = 0.01\n","    \n","    SKIP_FOLDS = [0,2,3]\n","    LOCAL_SEED = 0"],"metadata":{"id":"-Gq8LiUmdbwS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = pd.read_csv(f\"{CFG.INPUT}/train.csv\")\n","train_df.head()"],"metadata":{"id":"HhLSi6Y6ebtg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cv = MultilabelStratifiedKFold(n_splits=CFG.N_FOLD, shuffle=True, random_state=CFG.SEED)\n","for n, (train_index, valid_index) in enumerate(cv.split(train_df, train_df[CFG.TARGETS])):\n","    train_df.loc[valid_index, 'fold'] = int(n)\n","train_df['fold'] = train_df['fold'].astype(int)"],"metadata":{"id":"O547xsDoedqy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TOKENIZER = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\n","TOKENIZER.save_pretrained(CFG.OUTPUT+'tokenizer/')\n","CFG.TOKENIZER = TOKENIZER\n","del TOKENIZER"],"metadata":{"id":"pNZIcekTefsD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FB3Dataset(Dataset):\n","    def __init__(self, df):\n","        self.texts = df['full_text'].values\n","        self.labels = df[CFG.TARGETS].values\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, item):\n","        inputs = CFG.TOKENIZER.encode_plus(\n","            self.texts[item], \n","            return_tensors=None, \n","            add_special_tokens=True, \n","            max_length=CFG.MAX_LEN,\n","            pad_to_max_length=True,\n","            truncation=True\n","        )\n","        for k, v in inputs.items():\n","            inputs[k] = torch.tensor(v, dtype=torch.long) \n","        label = torch.tensor(self.labels[item], dtype=torch.float)\n","        return inputs, label\n","    \n","\n","def collate(inputs):\n","    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n","    for k, v in inputs.items():\n","        inputs[k] = inputs[k][:,:mask_len]\n","    return inputs"],"metadata":{"id":"iXjowusfehsq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MeanPooling(nn.Module):\n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        mean_embeddings = sum_embeddings / sum_mask\n","        return mean_embeddings\n","        \n","class CustomModel(nn.Module):\n","    def __init__(self, model_name):\n","        super().__init__()\n","\n","        self.config = AutoConfig.from_pretrained(model_name, output_hidden_states=True)\n","        self.config.hidden_dropout = 0.\n","        self.config.hidden_dropout_prob = 0.\n","        self.config.attention_dropout = 0.\n","        self.config.attention_probs_dropout_prob = 0.\n","\n","        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n","\n","        if CFG.GRAD_CHECKPOINT:\n","            self.model.gradient_checkpointing_enable()\n","            \n","        self.pool = MeanPooling()\n","        self.fc = nn.Linear(self.config.hidden_size, 6)\n","        self._init_weights(self.fc)\n","\n","        #self.n_self_fc = 2\n","        #lst = []\n","        #for _ in range(self.n_self_fc):\n","        #    l = nn.Linear(self.config.hidden_size, 6)\n","        #    self._init_weights(l)\n","        #    lst.append(l)\n","        #self.self_fcs = nn.ModuleList(lst)\n","        #self.self_hints = nn.ModuleList([nn.Linear(self.config.hidden_size, self.config.hidden_size) for _ in range(self.n_self_fc)])\n","\n","        # reinit_layers\n","        reinit_layer = 1\n","        if reinit_layer > 0:\n","            for layer in self.model.encoder.layer[-reinit_layer:]:\n","                for module in layer.modules():\n","                    if isinstance(module, nn.Linear):\n","                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","                        if module.bias is not None:\n","                            module.bias.data.zero_()\n","                    elif isinstance(module, nn.Embedding):\n","                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","                        if module.padding_idx is not None:\n","                            module.weight.data[module.padding_idx].zero_()\n","                    elif isinstance(module, nn.LayerNorm):\n","                        module.bias.data.zero_()\n","                        module.weight.data.fill_(1.0)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(self, inputs):\n","        outputs = self.model(**inputs)\n","        last_hidden_states = outputs[0]\n","        #use_layer = 2\n","        #sequence_output = torch.cat([outputs[1][-1*i] for i in range(1, use_layer+1)], dim=2)  # concatenate\n","        \n","        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n","        output = self.fc(feature)\n","\n","        #if self.training:\n","        #    features = [self.pool(outputs[1][-1*(i + 2)], inputs['attention_mask']) for i in range(self.n_self_fc)]\n","        #    outputs = [self.self_fcs[i](features[i]) for i in range(self.n_self_fc)]\n","        #    #hints = [self.self_hints[i](features[i]) for i in range(self.n_self_fc)]\n","        #    hints = None\n","        #else:\n","        #    outputs = None\n","        #    hints = None\n","        \n","        #return output,  last_hidden_states # sequence_output\n","        #return output, outputs, hints, feature\n","        return output"],"metadata":{"id":"qWyd6CohgZYC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        \n","def MCRMSE(y_trues, y_preds):\n","    scores = []\n","    idxes = y_trues.shape[1]\n","    for i in range(idxes):\n","        y_true = y_trues[:,i]\n","        y_pred = y_preds[:,i]\n","        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n","        scores.append(score)\n","    mcrmse_score = np.mean(scores)\n","    return mcrmse_score, scores\n","\n","\n","def get_score(y_trues, y_preds):\n","    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n","    return mcrmse_score, scores\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"],"metadata":{"id":"1IMrGYMJfdAO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_parameters = [\n","        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","         'lr': encoder_lr, 'weight_decay': weight_decay},\n","        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n","         'lr': encoder_lr, 'weight_decay': 0.0},\n","        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n","         'lr': decoder_lr, 'weight_decay': 0.0}\n","    ]\n","    return optimizer_parameters"],"metadata":{"id":"FugLmIjdfj1W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_fn(epoch, train_loader, teacher, student, criterion, kl_loss, optimizer, scheduler):\n","    student.train()\n","    #l2_loss = nn.MSELoss()\n","    #cos_loss = nn.CosineEmbeddingLoss()\n","\n","    losses = AverageMeter()\n","    global_step = 0\n","    for step, (inputs, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n","        inputs = collate(inputs)\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        \n","        batch_size = labels.size(0)\n","        with torch.cuda.amp.autocast(enabled=is_gpu):\n","            y_preds_s = student(inputs)\n","            #y_preds_s, h_s = student(inputs)\n","            #y_preds_s, h_preds = student(inputs)\n","            #y_preds_s, h_preds, hints, feat = student(inputs)\n","            #base_loss = criterion(y_preds_s, labels)\n","            #print(base_loss, nn.SmoothL1Loss(reduction='none')(y_preds_s, labels))\n","            \"\"\"\n","            with torch.no_grad():\n","                _loss1 = nn.L1Loss(reduction='none')(y_preds_s, labels)\n","            mask = (_loss1 < 1e-3).float()\n","            _lab = y_preds_s.clone().detach()\n","            #feat = feat.clone().detach()\n","            for p in h_preds:\n","            #for p, h in zip(h_preds, hints):\n","                _loss2 = nn.SmoothL1Loss(reduction='none')(p, _lab)\n","                #_loss3 = nn.SmoothL1Loss(reduction='none')(p, labels)\n","                base_loss += (mask * _loss2).mean()\n","                #base_loss += (mask * _loss3).mean()\n","\n","                #_loss1 = nn.SmoothL1Loss(reduction='mean')(p, labels).detach()\n","                #_loss2 = nn.SmoothL1Loss(reduction='none')(p, labels)\n","                #base_loss += (_loss2 * (_loss2.detach() < _loss1).float()).mean()\n","                #/ student.n_self_fc\n","                #base_loss += criterion(p, _lab) / student.n_self_fc\n","                _lab = p.clone().detach()\n","                #base_loss += criterion(p, labels) / student.n_self_fc / 8\n","                #base_loss += kl_loss(h, feat)\n","                #feat = h.clone().detach()\n","            \"\"\"\n","            with torch.no_grad():\n","                #y_preds_t, h_t = teacher(inputs)\n","                y_preds_t = teacher(inputs)\n","                #_loss1 = nn.L1Loss(reduction='none')(y_preds_t, labels)\n","            #mask = (_loss1 < 1e-2).float()\n","\n","            base_loss = criterion(y_preds_s, y_preds_t)\n","            #_loss2 = nn.SmoothL1Loss(reduction='none')(y_preds_s, y_preds_t)\n","            #base_loss = (mask * _loss2).mean()\n","\n","            loss = base_loss #+ kl_loss(y_preds_s, y_preds_t) + kl_loss(h_s, h_t)\n","            #loss = base_loss + kl_loss(y_preds_s, y_preds_t) + l2_loss(h_s, h_t)\n","            #dummy = torch.ones((labels.size(0),)).to(device)\n","            #loss = base_loss + kl_loss(y_preds_s, y_preds_t) + cos_loss(h_s.reshape(labels.size(0), -1), h_t.reshape(labels.size(0), -1), dummy)\n","\n","\n","        if CFG.ACCUMLATION > 1:\n","            loss = loss / CFG.ACCUMLATION\n","            \n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        scaler.unscale_(optimizer)\n","        grad_norm = torch.nn.utils.clip_grad_norm_(student.parameters(), CFG.GRAD_NORM)\n","        \n","        if (step + 1) % CFG.ACCUMLATION == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","            global_step += 1\n","            scheduler.step()\n","                \n","    return losses.avg\n","\n","def valid_fn(valid_loader, model, criterion):\n","    losses = AverageMeter()\n","    model.eval()\n","    preds = []\n","    for step, (inputs, labels) in enumerate(valid_loader):\n","        inputs = collate(inputs)\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.no_grad():\n","            y_preds = model(inputs)\n","            #y_preds, _ = model(inputs)\n","            #y_preds, _, _, _ = model(inputs)\n","            loss = criterion(y_preds, labels)\n","            \n","        if CFG.ACCUMLATION > 1:\n","            loss = loss / CFG.ACCUMLATION\n","        losses.update(loss.item(), batch_size)\n","        preds.append(y_preds.to('cpu').numpy())\n","\n","    predictions = np.concatenate(preds)\n","    return losses.avg, predictions"],"metadata":{"id":"hedRPSQnflVs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pseudo_dfs = []\n","for fold in range(CFG.N_FOLD):\n","    _df = pd.read_csv(f\"./pseudo_v2_f{fold}.csv\")\n","    _df = _df.rename(columns = {\"text\":\"full_text\"}).drop([\"ARI\", \"predicted_grade\", \"tokenize_length\"], axis=1)\n","    _df[\"fold\"] = fold\n","    _df[\"text_id\"] = [f\"pseudo_{i}\" for i in range(len(_df))]\n","    pseudo_dfs.append(_df)"],"metadata":{"id":"-wSlzorjgpXN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_loop(fold, seed):\n","    valid_df = train_df.query(f\"fold=={fold}\")\n","    valid_labels = valid_df[CFG.TARGETS].values\n","    #train_dataset = FB3Dataset(train_df.query(f\"fold!={fold}\"))\n","    train_dataset = FB3Dataset(pd.concat([train_df.query(f\"fold!={fold}\"), pseudo_dfs[fold]], axis=0).reset_index(drop=True))\n","    valid_dataset = FB3Dataset(valid_df)\n","    train_loader = DataLoader(train_dataset,\n","                              batch_size=CFG.BS,\n","                              shuffle=True,\n","                              num_workers=CFG.N_WORKER, pin_memory=True, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset,\n","                              batch_size=CFG.BS * 2,\n","                              shuffle=False,\n","                              num_workers=CFG.N_WORKER, pin_memory=True, drop_last=False)\n","\n","    #student = CustomModel(\"microsoft/deberta-v3-small\")\n","    student = CustomModel(\"microsoft/deberta-v3-xsmall\")\n","    teacher = CustomModel(CFG.MODEL_NAME)\n","\n","    torch.save(student.config, CFG.OUTPUT + 'config.pth')\n","    state = torch.load(f\"./microsoft-deberta-v3-base_seed0_fold{fold}_best.pth\",\n","                       map_location=torch.device('cpu'))\n","    teacher.load_state_dict(state['model'])\n","    student.to(device)\n","    teacher.to(device)\n","    teacher.eval()\n","\n","    optimizer_parameters = get_optimizer_params(student,\n","                                                encoder_lr=CFG.ENCODER_LR, \n","                                                decoder_lr=CFG.DECODER_LR,\n","                                                weight_decay=CFG.WEIGHT_DECAY)\n","\n","    optimizer = AdamW(optimizer_parameters, lr=CFG.ENCODER_LR, eps=CFG.EPS, betas=CFG.BETAS)\n","    num_train_steps = int(len(train_dataset) / CFG.BS * CFG.N_EPOCH)\n","    scheduler = get_cosine_schedule_with_warmup(\n","            optimizer, num_warmup_steps=CFG.N_WARMUP, num_training_steps=num_train_steps, num_cycles=CFG.N_CYCLES\n","    )\n","    criterion = nn.SmoothL1Loss(reduction='mean')\n","    kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n","\n","    best_score = float(\"inf\")\n","    best_predictions = None\n","    for epoch in range(CFG.N_EPOCH):\n","        avg_loss = train_fn(epoch, train_loader, teacher, student, criterion, kl_loss, optimizer, scheduler)\n","        avg_val_loss, predictions = valid_fn(valid_loader, student, criterion)\n","        score, _ = get_score(valid_labels, predictions)\n","        if best_score > score:\n","            best_score = score\n","            best_predictions = predictions\n","            torch.save({'model': student.state_dict(),\n","                        'predictions': predictions},\n","                         f\"{CFG.OUTPUT}/{CFG.MODEL_NAME.replace('/', '-')}_seed{seed}_fold{fold}_best.pth\")\n","        print(f\"[Fold-{fold}] epoch-{epoch}: score={score}\")\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","        \n","    return best_score"],"metadata":{"id":"2XPpKLddftcU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main(seed):\n","    seed_everything(seed)\n","    scores = []\n","    for fold in range(CFG.N_FOLD):\n","        if fold in CFG.SKIP_FOLDS:\n","            continue\n","        score = train_loop(fold, seed)\n","        scores.append(score)\n","    print(scores)\n","    print(sum(scores) / CFG.N_FOLD)"],"metadata":{"id":"Cxoi2AWjf3-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    main(CFG.LOCAL_SEED)"],"metadata":{"id":"z7TAsq4Df6bL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 通常の蒸留: 0.4586991475202113\n","# 普通のラベルも一緒に: 0.4610501424488637\n","# mask 0.2: 0.4920362723590115\n","\n","\n","# base; 0.4659324501443643, 0.4631171619592938\n","# ---------------------------------------\n","\n","# 1.0; 0.48xxx\n","# 1e-1; 0.47074394338866504\n","# 1e-2; 0.4646448016043702\n","# 1e-3; 0.4629622086679886, 0.4631343361664209\n","# 1e-4; 0.46315064678772244\n","\n","\n","# 4 layer; 0.46618104604628985\n","# 1 layer; 0.4647979115503782\n","\n","# label: 0.4633481539801743"],"metadata":{"id":"kP1ayVIscoZb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["```\n","[Fold-1] epoch-0: score=0.48272674656153364\n","100%\n","366/366 [06:13<00:00, 1.09s/it]\n","[Fold-1] epoch-1: score=0.47691091973256344\n","100%\n","366/366 [06:10<00:00, 1.17it/s]\n","[Fold-1] epoch-2: score=0.46621649542127847\n","100%\n","366/366 [06:14<00:00, 1.06s/it]\n","[Fold-1] epoch-3: score=0.4631343361664209\n","```\n","\n","```\n","[Fold-1] epoch-0: score=0.5017394744811813\n","100%\n","366/366 [06:10<00:00, 1.07s/it]\n","[Fold-1] epoch-1: score=0.493347700873547\n","100%\n","366/366 [06:07<00:00, 1.18it/s]\n","[Fold-1] epoch-2: score=0.4827352103163725\n","100%\n","366/366 [06:11<00:00, 1.05s/it]\n","[Fold-1] epoch-3: score=0.4791843935000248\n","```\n","\n","```\n","[Fold-1] epoch-0: score=0.634812305090922\n","100%\n","366/366 [06:10<00:00, 1.08s/it]\n","[Fold-1] epoch-1: score=0.5402783404093685\n","100%\n","366/366 [06:07<00:00, 1.19it/s]\n","[Fold-1] epoch-2: score=0.49520196558257107\n","100%\n","366/366 [06:11<00:00, 1.05s/it]\n","[Fold-1] epoch-3: score=0.4839299213206399\n","[0.4839299213206399]\n","0.12098248033015997\n","```\n","\n","```\n","[Fold-1] epoch-0: score=0.7845522330439376\n","100%\n","366/366 [06:04<00:00, 1.08it/s]\n","[Fold-1] epoch-1: score=0.5016520683400798\n","100%\n","366/366 [06:05<00:00, 1.07s/it]\n","[Fold-1] epoch-2: score=0.4919881528497781\n","100%\n","366/366 [06:12<00:00, 1.05it/s]\n","[Fold-1] epoch-3: score=0.48170858445447107\n","```"],"metadata":{"id":"7ctjo6K9dIdo"}},{"cell_type":"markdown","source":["\n","### スコアメモ\n","\n","#### basic lossのみ\n","\n","|epoch|score|\n","|--|--|\n","|0|0.526931088418688|\n","|1||\n","|2||\n","|3||\n","\n","#### basic loss + pseudo\n","\n","|epoch|score|\n","|--|--|\n","|0|0.46634102132671007|\n","|1|0.4600812197041982|\n","|2|0.46426920637470737|\n","|3|0.46183874646246315|\n","\n","#### basic loss + norm 0.01\n","\n","|epoch|score|\n","|--|--|\n","|0|0.5363891288129078|\n","|1|0.4848848712197243|\n","|2|0.4729763663353857|\n","|3|0.4659324501443643|\n","\n","#### basic loss + norm 1.0\n","\n","|epoch|score|\n","|--|--|\n","|0|0.5261090139783512|\n","|1|0.491587607694613|\n","|2|0.4890059951163896|\n","|3||\n","\n","\n","#### 自己蒸留\n","\n","#### 1層(LKloss)\n","\n","|epoch|score|\n","|--|--|\n","|0|0.6494795403379258|\n","|1|0.6xxx|\n","|2||\n","|3||\n","\n","#### 1層\n","\n","|epoch|score|\n","|--|--|\n","|0|0.5031831317162344|\n","|1||\n","|2||\n","|3||\n","\n","#### 2層\n","\n","|epoch|score|\n","|--|--|\n","|0|0.49955065566735096|\n","|1|0.5446511805141959|\n","|2|0.4856458101870027|\n","|3|0.4808126223513056|\n","\n","#### 2層 layer hint\n","\n","|epoch|score|\n","|--|--|\n","|0|0.5030051283685592|\n","|1|0.5550490815798361|\n","|2|0.4870029197380179|\n","|3|0.4815796639136917|\n","\n","#### 2層 layer hint KL\n","\n","|epoch|score|\n","|--|--|\n","|0|0.6493452335084183|\n","|1|0.6484945935918974|\n","|2||\n","|3||\n","\n","\n","- AVG: 0.47679568644182346\n","- AVG / 2: 0.474152817510402\n","- AVG / 4: 0.47159031839216814\n","\n","#### 4層\n","\n","|epoch|score|\n","|--|--|\n","|0|0.51080632591456|\n","|1|0.4972932551819587|\n","|2|0.50240882382299|\n","|3||\n","\n","#### ベースライン\n","\n","|epoch|score|\n","|--|--|\n","|0|2.2438404386854334|\n","|1|1.4579084189192848|\n","|2|1.145673294975656|\n","|3||\n","\n","#### ターゲットのKLは計算しない\n","\n","|epoch|score|\n","|--|--|\n","|0|2.9955258771870557|\n","|1|2.7831887114903076|\n","|2||\n","|3||\n","\n","#### L2 loss\n","\n","|epoch|score|\n","|--|--|\n","|0|7.229032346756473|\n","|1|11.169361000883642|\n","|2|13.013728330459623|\n","|3|13.306884086590356|\n","\n","#### cos loss\n","\n","|epoch|score|\n","|--|--|\n","|0|7.219968155264872|\n","|1|11.167937634035303|\n","|2||\n","|3||\n","\n","#### 各レイヤーのloss\n","\n","##### 4層\n","\n","|epoch|score|\n","|--|--|\n","|0|2.6595548899905315|\n","|1|2.207810980642428|\n","|2||\n","|3||\n","\n","##### 2層\n","\n","|epoch|score|\n","|--|--|\n","|0|2.5768557287045444|\n","|1|1.9825857271857064|\n","|2||\n","|3||\n","\n","##### feature層\n","\n","|epoch|score|\n","|--|--|\n","|0||\n","|1||\n","|2||\n","|3||\n","\n","#### Layerリセット\n","\n","##### 1層\n","\n","|epoch|score|\n","|--|--|\n","|0|2.031728988555573|\n","|1|1.2354769068980633|\n","|2||\n","|3||\n","\n","##### 1層+MSE\n","\n","|epoch|score|\n","|--|--|\n","|0|8.05171028512562|\n","|1||\n","|2||\n","|3||\n","\n","##### 2層\n","\n","|epoch|score|\n","|--|--|\n","|0|2.051898461291952|\n","|1||\n","|2||\n","|3||\n","\n","#### パラメータチューニング(1層リセット)\n","\n","encoder = 5e-5\n","\n","|epoch|score|\n","|--|--|\n","|0|2.0558456883621465|\n","|1|1.293154511884232|\n","|2|0.9928888108647462|\n","|3||\n","\n","encoder = 2e-4\n","\n","|epoch|score|\n","|--|--|\n","|0|2.053561709873904|\n","|1||\n","|2||\n","|3||\n","\n","#### CLSトークン\n","\n","|epoch|score|\n","|--|--|\n","|0||\n","|1||\n","|2||\n","|3||"],"metadata":{"id":"R2Kj_ENVCaIj"}},{"cell_type":"code","source":[],"metadata":{"id":"Um0do90PCXaN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9Ga9YDLXyY5V"},"execution_count":null,"outputs":[]}]}