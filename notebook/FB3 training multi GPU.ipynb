{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ff40f-912a-4f37-965f-5073646c5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e3a474-1bee-4a54-a99c-b148e3f9c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q kaggle\n",
    "!mkdir -p .kaggle\n",
    "!cp \"./kaggle.json\" .kaggle/\n",
    "!chmod 600 .kaggle/kaggle.json\n",
    "!cp -r .kaggle /root\n",
    "\n",
    "!kaggle -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d510233c-6cb6-4b72-b448-67df42254594",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install iterative-stratification==0.1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a471844-74b1-43f8-ae6b-ba3832b4d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad7e4d-4f94-44fe-bc50-596ce43826c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle datasets download -d takamichitoda/fb3-deberta-v3-pseudo\n",
    "#!kaggle kernels output takamichitoda/fb3-pseudo-train-3rd-fold-1-only\n",
    "#!mkdir -p pseudo_base\n",
    "#!unzip fb3-deberta-v3-pseudo.zip\n",
    "#!rm -rf fb3-deberta-v3-pseudo.zip microsoft-deberta-v3-base_seed0_fold1_best.pth fb3-pseudo-train-3rd-fold-1-only.log\n",
    "#!mv microsoft-deberta-v3-base_seed* pseudo_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba5982e-0360-4d5c-83b8-b6796d6c5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle competitions download -c feedback-prize-english-language-learning\n",
    "#!unzip feedback-prize-english-language-learning.zip\n",
    "#!mkdir feedback-prize-english-language-learning\n",
    "#!mv sample_submission.csv test.csv train.csv feedback-prize-english-language-learning/\n",
    "#!rm -rf feedback-prize-english-language-learning.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f69eb-7e9a-4d47-9344-625ed93cb33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir fb3-make-pseudo-label-4th\n",
    "#!kaggle kernels output takamichitoda/fb3-make-pseudo-label-4th -p fb3-make-pseudo-label-4th/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c275d2-4f3f-4e61-a6ae-2d9ffe632ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "#from accelerate import notebook_launcher\n",
    "#from accelerate.utils import set_seed\n",
    "#from accelerate import Accelerator\n",
    "\n",
    "is_gpu = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_gpu else 'cpu')\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=is_gpu)\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "print(device)\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff99732e-7792-470a-b49d-acfc0dfdfb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    EXP = \"exp143\"\n",
    "    INPUT = \"/home/jupyter/feedback-prize-english-language-learning\"\n",
    "    #OUTPUT = \"/home/jupyter/output/\"\n",
    "    OUTPUT = f\"/home/jupyter/{EXP}/\"\n",
    "    SEED = 0\n",
    "    N_FOLD = 4\n",
    "    TARGETS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    \n",
    "    #MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "    MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "    TOKENIZER = None\n",
    "    MAX_LEN = 1428\n",
    "    GRAD_CHECKPOINT = True\n",
    "    \n",
    "    N_EPOCH = 4 # 2\n",
    "    N_WORKER = 2\n",
    "    ENCODER_LR = 5e-6 # 2e-5\n",
    "    DECODER_LR = 2e-5\n",
    "    EPS = 1e-6\n",
    "    BETAS = (0.9, 0.999)\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    N_WARMUP = 0\n",
    "    N_CYCLES = 0.5\n",
    "    \n",
    "    BS = 8 #2 #4\n",
    "    ACCUMLATION = 1 # 4 #2\n",
    "    \n",
    "    GRAD_NORM = 0.1\n",
    "    \n",
    "    ADV_EPS = 1e-4\n",
    "    ADV_LR = 1e-4\n",
    "    ADV_START = 2\n",
    "    \n",
    "    FGM_EPS = 1e-1\n",
    "    FGM_END = float(\"inf\")\n",
    "    \n",
    "    SKIP_FOLDS = [1,2,3]\n",
    "    LOCAL_SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32370e8d-48f2-4f0b-a329-b8ec14301544",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{CFG.INPUT}/train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1063a-7a4a-4d76-a294-79ea60183452",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = MultilabelStratifiedKFold(n_splits=CFG.N_FOLD, shuffle=True, random_state=CFG.SEED)\n",
    "for n, (train_index, valid_index) in enumerate(cv.split(train_df, train_df[CFG.TARGETS])):\n",
    "    train_df.loc[valid_index, 'fold'] = int(n)\n",
    "train_df['fold'] = train_df['fold'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0405b2f-1618-4f1c-8ef1-0dae6cd3c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\n",
    "TOKENIZER.save_pretrained(CFG.OUTPUT+'tokenizer/')\n",
    "CFG.TOKENIZER = TOKENIZER\n",
    "del TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f9ec7-f0e3-4afd-af42-e839b22013b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FB3Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = df['full_text'].values\n",
    "        self.labels = df[CFG.TARGETS].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = CFG.TOKENIZER.encode_plus(\n",
    "            self.texts[item], \n",
    "            return_tensors=None, \n",
    "            add_special_tokens=True, \n",
    "            max_length=CFG.MAX_LEN,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long) \n",
    "        label = torch.tensor(self.labels[item], dtype=torch.float)\n",
    "        return inputs, label\n",
    "    \n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89115c61-98a1-49bc-8f65-813c256a5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(CFG.MODEL_NAME, output_hidden_states=True)\n",
    "        self.config.hidden_dropout = 0.\n",
    "        self.config.hidden_dropout_prob = 0.\n",
    "        self.config.attention_dropout = 0.\n",
    "        self.config.attention_probs_dropout_prob = 0.\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(CFG.MODEL_NAME, config=self.config)\n",
    "\n",
    "        if CFG.GRAD_CHECKPOINT:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "            \n",
    "        self.pool = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 6)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        \n",
    "        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "        output = self.fc(feature)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6878b9d0-7f2c-4822-941d-099bd63e58fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        adv_param=\"weight\",\n",
    "        adv_lr=1,\n",
    "        adv_eps=0.2,\n",
    "        start_epoch=0,\n",
    "        adv_step=1,\n",
    "        scaler=None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.start_epoch = start_epoch\n",
    "        self.adv_step = adv_step\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def attack_backward(self, inputs, labels, epoch):\n",
    "        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n",
    "            return None\n",
    "\n",
    "        self._save() \n",
    "        for i in range(self.adv_step):\n",
    "            self._attack_step() \n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_preds = self.model(inputs)\n",
    "                adv_loss = self.criterion(y_preds, labels)\n",
    "                #adv_loss = hinge_loss(y_preds, labels, 0.025)\n",
    "                \n",
    "            #y_preds = self.model(inputs)\n",
    "            #adv_loss = self.criterion(y_preds, labels)\n",
    "            #accelerator.backward(adv_loss)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(adv_loss).backward()\n",
    "            \n",
    "        self._restore()\n",
    "\n",
    "    def _attack_step(self):\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
    "                    )\n",
    "                # param.data.clamp_(*self.backup_eps[name])\n",
    "\n",
    "    def _save(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(self,):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4e3904-4e48-4a72-886b-ac1a679605cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/143764\n",
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon=1., emb_name='word_embeddings'):\n",
    "        \"\"\"\n",
    "        敵対的な摂動を求め、現在のembedding layerに摂動を加える\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "                    \n",
    "    def restore(self, emb_name='word_embeddings'):\n",
    "        \"\"\"\n",
    "        敵対的な摂動を求める際に変更してしまったembedding layerのパラメータについて\n",
    "        元のパラメータを代入する\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "            self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07edfad5-2063-4573-bc4a-9b34586680b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def MCRMSE(y_trues, y_preds):\n",
    "    scores = []\n",
    "    idxes = y_trues.shape[1]\n",
    "    for i in range(idxes):\n",
    "        y_true = y_trues[:,i]\n",
    "        y_pred = y_preds[:,i]\n",
    "        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n",
    "        scores.append(score)\n",
    "    mcrmse_score = np.mean(scores)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_score(y_trues, y_preds):\n",
    "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a1806b-ce21-4962-a2fc-452669f33fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "         'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cca8634-792a-4735-b866-3f7cf5496f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "\n",
    "class PriorWD(Optimizer):\n",
    "    def __init__(self, optim, use_prior_wd=False, exclude_last_group=True):\n",
    "        super(PriorWD, self).__init__(optim.param_groups, optim.defaults)\n",
    "        self.param_groups = optim.param_groups\n",
    "        self.optim = optim\n",
    "        self.use_prior_wd = use_prior_wd\n",
    "        self.exclude_last_group = exclude_last_group\n",
    "        self.weight_decay_by_group = []\n",
    "        for i, group in enumerate(self.param_groups):\n",
    "            self.weight_decay_by_group.append(group[\"weight_decay\"])\n",
    "            group[\"weight_decay\"] = 0\n",
    "\n",
    "        self.prior_params = {}\n",
    "        for i, group in enumerate(self.param_groups):\n",
    "            for p in group[\"params\"]:\n",
    "                self.prior_params[id(p)] = p.detach().clone()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        if self.use_prior_wd:\n",
    "            for i, group in enumerate(self.param_groups):\n",
    "                for p in group[\"params\"]:\n",
    "                    if self.exclude_last_group and i == len(self.param_groups):\n",
    "                        p.data.add_(-group[\"lr\"] * self.weight_decay_by_group[i], p.data)\n",
    "                    else:\n",
    "                        p.data.add_(\n",
    "                            -group[\"lr\"] * self.weight_decay_by_group[i], p.data - self.prior_params[id(p)],\n",
    "                        )\n",
    "        loss = self.optim.step(closure)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def compute_distance_to_prior(self, param):\n",
    "        assert id(param) in self.prior_params, \"parameter not in PriorWD optimizer\"\n",
    "        return (param.data - self.prior_params[id(param)]).pow(2).sum().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb66a3cb-d3da-496c-9077-251b464a5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def hinge_loss(y_pred, y_true):\n",
    "#    return torch.mean(torch.clamp(1 - y_pred.t() * y_true, min=0))\n",
    "\n",
    "#eps = 0.25\n",
    "def hinge_loss(y_pred, y_true, eps):\n",
    "    loss = nn.L1Loss(reduction='none')(y_pred, y_true)\n",
    "    loss = torch.clamp(loss - eps, min=0)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def mask_loss(y_pred, y_true):\n",
    "    loss = nn.SmoothL1Loss(reduction='none', beta=1.0)(y_pred, y_true)\n",
    "    mask = (nn.L1Loss(reduction='none')(y_pred, y_true) < 0.5).float()\n",
    "    loss = loss * mask\n",
    "    loss = torch.mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211037e-2c1e-46cd-8d6d-346fd702f09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(epoch, train_loader, model, awp, fgm, criterion, optimizer, scheduler):\n",
    "    model.train()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "            #inputs[k] = v.to(accelerator.device)\n",
    "        #labels = labels.to(accelerator.device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=is_gpu):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "            #loss = hinge_loss(y_preds, labels, 0.025)\n",
    "        #y_preds = model(inputs)\n",
    "        #loss = criterion(y_preds, labels)\n",
    "            \n",
    "        if CFG.ACCUMLATION > 1:\n",
    "            loss = loss / CFG.ACCUMLATION\n",
    "            \n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        #accelerator.backward(loss)\n",
    "        \n",
    "        if awp is not None:\n",
    "            awp.attack_backward(inputs, labels, epoch) \n",
    "        \n",
    "        # FGM\n",
    "        if fgm is not None and epoch < CFG.FGM_END:\n",
    "            fgm.attack(epsilon=CFG.FGM_EPS, emb_name='word_embeddings')\n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_preds = model(inputs)\n",
    "                loss_adv = criterion(y_preds, labels)\n",
    "                #loss_adv = hinge_loss(y_preds, labels, 0.025)\n",
    "            #y_preds = model(inputs)\n",
    "            #loss_adv = criterion(y_preds, labels)\n",
    "            #optimizer.zero_grad()\n",
    "            scaler.scale(loss_adv).backward()\n",
    "            #accelerator.backward(loss_adv)\n",
    "            fgm.restore()\n",
    "        #\"\"\"\n",
    "\n",
    "        \n",
    "        if (step + 1) % CFG.ACCUMLATION == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.GRAD_NORM)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            #accelerator.clip_grad_norm_(model.parameters(), max_norm=CFG.GRAD_NORM, norm_type=2)\n",
    "            #optimizer.step()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            scheduler.step()\n",
    "                \n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "            \n",
    "        if CFG.ACCUMLATION > 1:\n",
    "            loss = loss / CFG.ACCUMLATION\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c6294-193a-4e62-a774-1ba45429fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_dfs = []\n",
    "for fold in range(CFG.N_FOLD):\n",
    "    _df = pd.read_csv(f\"./fb3-make-pseudo-label-4th/pseudo_v3_f{fold}.csv\")\n",
    "    _df = _df.rename(columns = {\"text\":\"full_text\"}).drop([\"ARI\", \"predicted_grade\", \"tokenize_length\"], axis=1)\n",
    "    _df[\"fold\"] = fold\n",
    "    _df[\"text_id\"] = [f\"pseudo_{i}\" for i in range(len(_df))]\n",
    "    pseudo_dfs.append(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee9faee-84e9-4d85-a2b4-e553783c7f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(fold, seed):\n",
    "    #set_seed(CFG.SEED)\n",
    "    #seed_everything(seed)\n",
    "    #accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    \n",
    "    valid_df = train_df.query(f\"fold=={fold}\")\n",
    "    valid_labels = valid_df[CFG.TARGETS].values\n",
    "    train_dataset = FB3Dataset(train_df.query(f\"fold!={fold}\"))\n",
    "    #train_dataset = FB3Dataset(pseudo_dfs[fold])\n",
    "    #train_dataset = FB3Dataset(pd.concat([train_df.query(f\"fold!={fold}\"), pseudo_dfs[fold]], axis=0).reset_index(drop=True))\n",
    "    valid_dataset = FB3Dataset(valid_df)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.BS,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.N_WORKER, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.BS,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.N_WORKER, pin_memory=True, drop_last=False)\n",
    "\n",
    "    model = CustomModel()\n",
    "    torch.save(model.config, CFG.OUTPUT + 'config.pth')\n",
    "    \n",
    "    s = 2 if fold == 1 else 0\n",
    "    path = f\"pseudo_base/microsoft-deberta-v3-base_seed{s}_fold{fold}_best.pth\"   \n",
    "    #path = f\"pseudo/microsoft-deberta-v3-large_seed0_fold{fold}_best.pth\"   \n",
    "    state = torch.load(path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.ENCODER_LR, \n",
    "                                                decoder_lr=CFG.DECODER_LR,\n",
    "                                                weight_decay=CFG.WEIGHT_DECAY)\n",
    "\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.ENCODER_LR, eps=CFG.EPS, betas=CFG.BETAS, correct_bias=True)\n",
    "    optimizer = PriorWD(optimizer, use_prior_wd=True)\n",
    "    \n",
    "    num_train_steps = int(len(train_dataset) / CFG.BS * CFG.N_EPOCH)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=CFG.N_WARMUP, num_training_steps=num_train_steps, num_cycles=CFG.N_CYCLES\n",
    "    )\n",
    "\n",
    "    #model, optimizer, train_loader, valid_loader, scheduler = accelerator.prepare(\n",
    "    #    model, optimizer, train_loader, valid_loader, scheduler\n",
    "    #)\n",
    "    \n",
    "    #criterion = nn.SmoothL1Loss(reduction='mean', beta=1.0)\n",
    "    criterion = mask_loss\n",
    "    #scaler = None\n",
    "    #\"\"\"\n",
    "    awp = AWP(model,\n",
    "              optimizer,\n",
    "              criterion,\n",
    "              adv_lr=CFG.ADV_LR,\n",
    "              adv_eps=CFG.ADV_EPS,\n",
    "              start_epoch=CFG.ADV_START,\n",
    "              scaler=scaler\n",
    "    )\n",
    "    #\"\"\"\n",
    "    #awp = None\n",
    "    fgm = FGM(model)\n",
    "    #\"\"\"\n",
    "    #awp, fgm = None, None\n",
    "\n",
    "    best_score = float(\"inf\")\n",
    "    best_predictions = None\n",
    "    for epoch in range(CFG.N_EPOCH):\n",
    "        avg_loss = train_fn(epoch, train_loader, model, awp, fgm, criterion, optimizer, scheduler)\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion)\n",
    "        score, _ = get_score(valid_labels, predictions)\n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            best_predictions = predictions\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                         f\"{CFG.OUTPUT}/{CFG.MODEL_NAME.replace('/', '-')}_seed{seed}_fold{fold}_best.pth\")\n",
    "        print(f\"[Fold-{fold}] epoch-{epoch}: score={score}\")\n",
    "        #accelerator.print(f\"[Fold-{fold}] epoch-{epoch}: score={score}\")\n",
    "        pd.DataFrame([score], columns=[\"score\"]).to_csv(f\"{CFG.OUTPUT}/fold{fold}_epoch{epoch}.csv\", index=None)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a7cac-e466-47d6-a0f0-ff7c85b734ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_loop(fold, seed):\n",
    "    set_seed(CFG.SEED)\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    accelerator.print(\"Fold\", fold)\n",
    "    accelerator.print(\"seed\", seed)\n",
    "    accelerator.print(accelerator.device)\n",
    "\n",
    "#args = (0, 0)\n",
    "#notebook_launcher(train_loop, args, num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b814560b-f576-42a6-9685-bdf6cf6f2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(seed):\n",
    "    seed_everything(seed)\n",
    "    scores = []\n",
    "    for fold in range(CFG.N_FOLD):\n",
    "        if fold in CFG.SKIP_FOLDS:\n",
    "            continue\n",
    "        score = train_loop(fold, seed)\n",
    "        scores.append(score)\n",
    "    print(scores)\n",
    "    print(sum(scores) / CFG.N_FOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c7488d-de0c-447a-a41f-4121cd02fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(CFG.LOCAL_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5388be1e-7a67-4260-92b7-6b360315074a",
   "metadata": {},
   "source": [
    "# Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dc57a6-4269-491e-ac38-7a5e5a50f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets init -p {CFG.OUTPUT}\n",
    "\n",
    "with open(f\"{CFG.OUTPUT}/dataset-metadata.json\", \"r\") as f:\n",
    "    d = json.load(f)\n",
    "    \n",
    "t = f\"FB3 {CFG.EXP} output\"\n",
    "d['title'] = t\n",
    "d['id'] = \"takamichitoda/\"+\"-\".join(t.split())\n",
    "\n",
    "with open(f\"{CFG.OUTPUT}/dataset-metadata.json\", \"w\") as f:\n",
    "    json.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07748a1-a96a-4e47-949a-f6e737955305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle datasets create -p {CFG.OUTPUT}\n",
    "!kaggle datasets version -m \"test\" -p {CFG.OUTPUT}/\n",
    "\n",
    "!kaggle datasets list -m --sort-by \"updated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b047d-80a8-4c10-8c93-0c212b71142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9868603e-46a7-43e7-93e4-15a5a0648625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
