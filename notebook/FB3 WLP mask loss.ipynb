{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a5d87b-d5d4-4486-a353-88284be30804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 14 09:13:51 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   61C    P0    28W /  70W |      0MiB / 15360MiB |     10%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7255a110-17df-4771-bf1d-f9a2bdb8691d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle API 1.5.12\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p .kaggle\n",
    "!cp \"./kaggle.json\" .kaggle/\n",
    "!chmod 600 .kaggle/kaggle.json\n",
    "!cp -r .kaggle /root\n",
    "\n",
    "!kaggle -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68a6b01-ae1b-4665-9595-f731568ab779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting iterative-stratification==0.1.7\n",
      "  Downloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.0.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.7) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.7) (3.1.0)\n",
      "Installing collected packages: iterative-stratification\n",
      "Successfully installed iterative-stratification-0.1.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install iterative-stratification==0.1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6281df0d-d301-4bd9-859d-08165783c6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n",
      "transformers.__version__: 4.20.1\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "is_gpu = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_gpu else 'cpu')\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=is_gpu)\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a82aabb7-f424-4f63-b131-a84035f1441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    EXP = \"exp197\"\n",
    "    INPUT = \"/home/jupyter/feedback-prize-english-language-learning\"\n",
    "    OUTPUT = f\"/home/jupyter/{EXP}/\"\n",
    "    SEED = 0\n",
    "    N_FOLD = 4\n",
    "    TARGETS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "\n",
    "    #MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "    MODEL_NAME = \"funnel-transformer/small\"\n",
    "    TOKENIZER = None\n",
    "    MAX_LEN = 768  # 1428\n",
    "    GRAD_CHECKPOINT = False\n",
    "    \n",
    "    N_EPOCH = 4\n",
    "    N_WORKER = 4\n",
    "    ENCODER_LR = 1e-5\n",
    "    DECODER_LR = 1e-4\n",
    "    EPS = 1e-6\n",
    "    BETAS = (0.9, 0.999)\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    N_WARMUP = 0\n",
    "    N_CYCLES = 0.5\n",
    "    \n",
    "    BS = 8\n",
    "    ACCUMLATION = 1\n",
    "    \n",
    "    GRAD_NORM = 0.1\n",
    "    \n",
    "    ADV_EPS = 1e-4\n",
    "    ADV_LR = 1e-4\n",
    "    ADV_START = 2\n",
    "    \n",
    "    FGM_EPS = 1e-1\n",
    "    FGM_END = float(\"inf\")\n",
    "    \n",
    "    SKIP_FOLDS = [None]\n",
    "    LOCAL_SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4466da71-3fe8-47cc-99df-3638d530dde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>The best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>Small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n",
       "3  003885A45F42  The best time in life is when you become yours...       4.5   \n",
       "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  fold  \n",
       "0     3.5         3.0          3.0      4.0          3.0     2  \n",
       "1     2.5         3.0          2.0      2.0          2.5     3  \n",
       "2     3.5         3.0          3.0      3.0          2.5     0  \n",
       "3     4.5         4.5          4.5      4.0          5.0     0  \n",
       "4     3.0         3.0          3.0      2.5          2.5     2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(f\"{CFG.INPUT}/train.csv\")\n",
    "\n",
    "cv = MultilabelStratifiedKFold(n_splits=CFG.N_FOLD, shuffle=True, random_state=CFG.SEED)\n",
    "for n, (train_index, valid_index) in enumerate(cv.split(train_df, train_df[CFG.TARGETS])):\n",
    "    train_df.loc[valid_index, 'fold'] = int(n)\n",
    "train_df['fold'] = train_df['fold'].astype(int)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9211d70-0781-4866-9438-6c8dc680cc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMASKING_TOKENS_FIXED = [\\n    (\"Generic_Name\", \"GENERIC_NAME\"),\\n    (\"Genric_Name\", \"GENERIC_NAME\"),\\n    (\"Generic_City\", \"GENERIC_CITY\"),\\n    (\"STUDENT_NAME\", \"STUDENT_NAME\"),\\n    (\"CLASS_NAME\", \"CLASS_NAME\"),\\n    (\"HOTEL_NAME\", \"HOTEL_NAME\"),\\n    (\"LOCATION_NAME\", \"LOCATION_NAME\"),\\n    (\"TEACHER_NAME\", \"TEACHER_NAME\"),\\n    (\"PROPER_NAME\", \"PROPER_NAME\"),\\n    (\"PROEPR_NAME\", \"PROPER_NAME\"),\\n    (\"OTHER_NAME\", \"OTHER_NAME\"),\\n    (\"SCHOOL_NAME\", \"SCHOOL_NAME\"),\\n    (\"STORE_NAME\", \"STORE_NAME\"),\\n    (\"Generic_School\", \"GENERIC_SCHOOL\"),\\n    (\"Generic_school\", \"GENERIC_SCHOOL\"),\\n    (\"RESTAURANT_NAME\", \"RESTAURANT_NAME\"),\\n    (\"LANGUAGE_NAME\", \"LANGUAGE_NAME\"),\\n]\\n\\nlst = []\\nfor i, full_text in enumerate(train_df[\"full_text\"]):\\n    for m, n in MASKING_TOKENS_FIXED:\\n        full_text = full_text.replace(m, n)\\n    lst.append(full_text)\\ntrain_df[\\'full_text\\'] = lst\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df[\"full_text\"] = train_df[\"full_text\"].map(lambda x: x.replace('\\r\\n', '[BR]'))\n",
    "#train_df[\"full_text\"] = train_df[\"full_text\"].map(lambda x: x.replace('\\n', '[BR]'))\n",
    "\n",
    "\"\"\"\n",
    "MASKING_TOKENS_FIXED = [\n",
    "    (\"Generic_Name\", \"GENERIC_NAME\"),\n",
    "    (\"Genric_Name\", \"GENERIC_NAME\"),\n",
    "    (\"Generic_City\", \"GENERIC_CITY\"),\n",
    "    (\"STUDENT_NAME\", \"STUDENT_NAME\"),\n",
    "    (\"CLASS_NAME\", \"CLASS_NAME\"),\n",
    "    (\"HOTEL_NAME\", \"HOTEL_NAME\"),\n",
    "    (\"LOCATION_NAME\", \"LOCATION_NAME\"),\n",
    "    (\"TEACHER_NAME\", \"TEACHER_NAME\"),\n",
    "    (\"PROPER_NAME\", \"PROPER_NAME\"),\n",
    "    (\"PROEPR_NAME\", \"PROPER_NAME\"),\n",
    "    (\"OTHER_NAME\", \"OTHER_NAME\"),\n",
    "    (\"SCHOOL_NAME\", \"SCHOOL_NAME\"),\n",
    "    (\"STORE_NAME\", \"STORE_NAME\"),\n",
    "    (\"Generic_School\", \"GENERIC_SCHOOL\"),\n",
    "    (\"Generic_school\", \"GENERIC_SCHOOL\"),\n",
    "    (\"RESTAURANT_NAME\", \"RESTAURANT_NAME\"),\n",
    "    (\"LANGUAGE_NAME\", \"LANGUAGE_NAME\"),\n",
    "]\n",
    "\n",
    "lst = []\n",
    "for i, full_text in enumerate(train_df[\"full_text\"]):\n",
    "    for m, n in MASKING_TOKENS_FIXED:\n",
    "        full_text = full_text.replace(m, n)\n",
    "    lst.append(full_text)\n",
    "train_df['full_text'] = lst\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11f1d229-abd5-43f7-a013-3aadb6bf3d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\n",
    "\n",
    "#new_tokens = list(set([j for i, j in MASKING_TOKENS_FIXED])) + [\"[BR]\"]\n",
    "#new_tokens = [\"[BR]\"]\n",
    "#TOKENIZER.add_tokens(list(new_tokens))\n",
    "\n",
    "TOKENIZER.save_pretrained(CFG.OUTPUT+'tokenizer/')\n",
    "CFG.TOKENIZER = TOKENIZER\n",
    "del TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce162c40-387b-461d-aeba-6f94dcd14b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b26356d29043b48c22823d942436c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3911 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1435\n",
      "before: 768\n",
      "after: 768\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "lengths = []\n",
    "tk0 = tqdm(train_df['full_text'].fillna(\"\").values, total=len(train_df))\n",
    "for text in tk0:\n",
    "    length = len(CFG.TOKENIZER(text, add_special_tokens=False)['input_ids'])\n",
    "    lengths.append(length)\n",
    "new_max_l = max(lengths) + 2 # cls & sep\n",
    "print(new_max_l)\n",
    "\n",
    "print('before:', CFG.MAX_LEN)\n",
    "#CFG.MAX_LEN = new_max_l\n",
    "print('after:', CFG.MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25ce0227-e1df-4bb0-972e-cb0fdfb8e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FB3Dataset(Dataset):\n",
    "    def __init__(self, df, train=True):\n",
    "        self.texts = df['full_text'].values\n",
    "        self.labels = df[CFG.TARGETS].values\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = CFG.TOKENIZER.encode_plus(\n",
    "            self.texts[item], \n",
    "            return_tensors=None, \n",
    "            add_special_tokens=True, \n",
    "            max_length=CFG.MAX_LEN, #if self.train else 768,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long) \n",
    "        label = torch.tensor(self.labels[item], dtype=torch.float)\n",
    "        return inputs, label\n",
    "    \n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8551082c-a1f7-44be-af30-66ae3c17b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(CFG.MODEL_NAME, output_hidden_states=True)\n",
    "        self.config.hidden_dropout = 0.\n",
    "        self.config.hidden_dropout_prob = 0.\n",
    "        self.config.attention_dropout = 0.\n",
    "        self.config.attention_probs_dropout_prob = 0.\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(CFG.MODEL_NAME, config=self.config)\n",
    "\n",
    "        if CFG.GRAD_CHECKPOINT:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        \n",
    "        self.layer_pool = None\n",
    "        self.pool = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 6)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        \n",
    "        pooling_embeddings = outputs[0]\n",
    "        \n",
    "        #all_hidden_states = torch.stack(outputs[1])\n",
    "        #pooling_embeddings = self.layer_pool(all_hidden_states)\n",
    "        \n",
    "        feature = self.pool(pooling_embeddings, inputs['attention_mask'])\n",
    "        output = self.fc(feature)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n",
    "            )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59d7bf35-f97a-4b48-bc59-2dcab6d59a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        adv_param=\"weight\",\n",
    "        adv_lr=1,\n",
    "        adv_eps=0.2,\n",
    "        start_epoch=0,\n",
    "        adv_step=1,\n",
    "        scaler=None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.start_epoch = start_epoch\n",
    "        self.adv_step = adv_step\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def attack_backward(self, inputs, labels, epoch):\n",
    "        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n",
    "            return None\n",
    "\n",
    "        self._save() \n",
    "        for i in range(self.adv_step):\n",
    "            self._attack_step() \n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_preds = self.model(inputs)\n",
    "                adv_loss = self.criterion(y_preds, labels)\n",
    "                \n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(adv_loss).backward()\n",
    "            \n",
    "        self._restore()\n",
    "\n",
    "    def _attack_step(self):\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
    "                    )\n",
    "                # param.data.clamp_(*self.backup_eps[name])\n",
    "\n",
    "    def _save(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(self,):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51fcc4ad-9cbb-43ea-8084-1033a116d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/143764\n",
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon=1., emb_name='word_embeddings'):\n",
    "        \"\"\"\n",
    "        敵対的な摂動を求め、現在のembedding layerに摂動を加える\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "                    \n",
    "    def restore(self, emb_name='word_embeddings'):\n",
    "        \"\"\"\n",
    "        敵対的な摂動を求める際に変更してしまったembedding layerのパラメータについて\n",
    "        元のパラメータを代入する\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "            self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca222f10-6a3c-4b5a-8724-156a68715edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def MCRMSE(y_trues, y_preds):\n",
    "    scores = []\n",
    "    idxes = y_trues.shape[1]\n",
    "    for i in range(idxes):\n",
    "        y_true = y_trues[:,i]\n",
    "        y_pred = y_preds[:,i]\n",
    "        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n",
    "        scores.append(score)\n",
    "    mcrmse_score = np.mean(scores)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_score(y_trues, y_preds):\n",
    "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afca21d3-dc6f-4456-bc18-d1c968a2b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "         'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ff45896-4223-4c49-9b67-82ab39e5f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "\n",
    "class PriorWD(Optimizer):\n",
    "    def __init__(self, optim, use_prior_wd=False, exclude_last_group=True):\n",
    "        super(PriorWD, self).__init__(optim.param_groups, optim.defaults)\n",
    "        self.param_groups = optim.param_groups\n",
    "        self.optim = optim\n",
    "        self.use_prior_wd = use_prior_wd\n",
    "        self.exclude_last_group = exclude_last_group\n",
    "        self.weight_decay_by_group = []\n",
    "        for i, group in enumerate(self.param_groups):\n",
    "            self.weight_decay_by_group.append(group[\"weight_decay\"])\n",
    "            group[\"weight_decay\"] = 0\n",
    "\n",
    "        self.prior_params = {}\n",
    "        for i, group in enumerate(self.param_groups):\n",
    "            for p in group[\"params\"]:\n",
    "                self.prior_params[id(p)] = p.detach().clone()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        if self.use_prior_wd:\n",
    "            for i, group in enumerate(self.param_groups):\n",
    "                for p in group[\"params\"]:\n",
    "                    if self.exclude_last_group and i == len(self.param_groups):\n",
    "                        p.data.add_(-group[\"lr\"] * self.weight_decay_by_group[i], p.data)\n",
    "                    else:\n",
    "                        p.data.add_(\n",
    "                            -group[\"lr\"] * self.weight_decay_by_group[i], p.data - self.prior_params[id(p)],\n",
    "                        )\n",
    "        loss = self.optim.step(closure)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def compute_distance_to_prior(self, param):\n",
    "        assert id(param) in self.prior_params, \"parameter not in PriorWD optimizer\"\n",
    "        return (param.data - self.prior_params[id(param)]).pow(2).sum().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b4ad0d4-f4d0-4590-b3b1-bcf394d04435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(epoch, train_loader, model, awp, fgm, criterion, optimizer, scheduler):\n",
    "    model.train()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=is_gpu):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "            \n",
    "        if CFG.ACCUMLATION > 1:\n",
    "            loss = loss / CFG.ACCUMLATION\n",
    "            \n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        awp.attack_backward(inputs, labels, epoch) \n",
    "        \n",
    "        # FGM\n",
    "        if epoch < CFG.FGM_END:\n",
    "            fgm.attack(epsilon=CFG.FGM_EPS, emb_name='word_embeddings')\n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_preds = model(inputs)\n",
    "                loss_adv = criterion(y_preds, labels)\n",
    "            #optimizer.zero_grad()\n",
    "            scaler.scale(loss_adv).backward()\n",
    "            fgm.restore()          \n",
    "        \n",
    "        scaler.unscale_(optimizer)\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.GRAD_NORM)\n",
    "        \n",
    "        if (step + 1) % CFG.ACCUMLATION == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            scheduler.step()\n",
    "                \n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "            \n",
    "        if CFG.ACCUMLATION > 1:\n",
    "            loss = loss / CFG.ACCUMLATION\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64e3042a-81ec-4d4f-a1f9-be2731dd6c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss(y_pred, y_true):\n",
    "    mask = nn.L1Loss(reduction='none')(y_pred, y_true) < 1.5\n",
    "    loss = nn.MSELoss(reduction='none')(y_pred, y_true)\n",
    "    loss = torch.mean(loss * mask.float())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8945c2bc-2a50-469b-a6b6-36b0d731c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(fold, seed):\n",
    "    valid_df = train_df.query(f\"fold=={fold}\")\n",
    "    valid_labels = valid_df[CFG.TARGETS].values\n",
    "    train_dataset = FB3Dataset(train_df.query(f\"fold!={fold}\"))\n",
    "    valid_dataset = FB3Dataset(valid_df)\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.BS,\n",
    "                              shuffle=True,\n",
    "                              #collate_fn=DataCollatorWithPadding(tokenizer=CFG.TOKENIZER, padding='longest'),\n",
    "                              num_workers=CFG.N_WORKER, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.BS,\n",
    "                              shuffle=False,\n",
    "                              #collate_fn=DataCollatorWithPadding(tokenizer=CFG.TOKENIZER, padding='longest'),\n",
    "                              num_workers=CFG.N_WORKER, pin_memory=True, drop_last=False)\n",
    "\n",
    "    model = CustomModel()\n",
    "    torch.save(model.config, CFG.OUTPUT + 'config.pth')\n",
    "\n",
    "    # load pre-train model\n",
    "    path = f\"pseudo_base/microsoft-deberta-v3-base_seed2_fold{fold}_best.pth\"   \n",
    "    state = torch.load(path, map_location=torch.device('cpu'))\n",
    "    #model.load_state_dict(state['model'])\n",
    "    \n",
    "    \n",
    "    # WeightedLayerPooling\n",
    "    #layer_start = 12\n",
    "    #model.layer_pool = WeightedLayerPooling(\n",
    "    #    model.config.num_hidden_layers, \n",
    "    #    layer_start=layer_start, layer_weights=None\n",
    "    #)\n",
    "    \n",
    "    # update tokens\n",
    "    #model.model.resize_token_embeddings(len(CFG.TOKENIZER))\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.ENCODER_LR, \n",
    "                                                decoder_lr=CFG.DECODER_LR,\n",
    "                                                weight_decay=CFG.WEIGHT_DECAY)\n",
    "\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.ENCODER_LR, eps=CFG.EPS, betas=CFG.BETAS, correct_bias=True)\n",
    "    optimizer = PriorWD(optimizer, use_prior_wd=True)\n",
    "    \n",
    "    num_train_steps = int(len(train_dataset) / CFG.BS * CFG.N_EPOCH)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=CFG.N_WARMUP, num_training_steps=num_train_steps, num_cycles=CFG.N_CYCLES\n",
    "    )\n",
    "\n",
    "    criterion = nn.SmoothL1Loss(reduction='mean', beta=1.0)\n",
    "    #criterion = svm_loss\n",
    "    \n",
    "    awp = AWP(model,\n",
    "              optimizer,\n",
    "              criterion,\n",
    "              adv_lr=CFG.ADV_LR,\n",
    "              adv_eps=CFG.ADV_EPS,\n",
    "              start_epoch=CFG.ADV_START,\n",
    "              scaler=scaler\n",
    "    )\n",
    "    fgm = FGM(model)\n",
    "\n",
    "    best_score = float(\"inf\")\n",
    "    best_predictions = None\n",
    "    for epoch in range(CFG.N_EPOCH):\n",
    "        avg_loss = train_fn(epoch, train_loader, model, awp, fgm, criterion, optimizer, scheduler)\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion)\n",
    "\n",
    "        score, _ = get_score(valid_labels, predictions)\n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            best_predictions = predictions\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                         f\"{CFG.OUTPUT}/{CFG.MODEL_NAME.replace('/', '-')}_seed{seed}_fold{fold}_best.pth\")\n",
    "        print(f\"[Fold-{fold}] epoch-{epoch}: score={score}\")\n",
    "        \n",
    "        pd.DataFrame([score], columns=[\"score\"]).to_csv(f\"{CFG.OUTPUT}/fold{fold}_epoch{epoch}.csv\", index=None)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd069237-e7f4-4726-9ced-dd2fb370d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(seed):\n",
    "    scores = []\n",
    "    for fold in range(CFG.N_FOLD):\n",
    "        if fold in CFG.SKIP_FOLDS:\n",
    "            continue\n",
    "        seed_everything(seed)\n",
    "        score = train_loop(fold, seed)\n",
    "        scores.append(score)\n",
    "    print(scores)\n",
    "    print(sum(scores) / CFG.N_FOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8395ef90-22f1-4253-9dcb-dd2a9d968e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752cc4d930cb47e6881e1fe8ec0f8548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold-0] epoch-0: score=0.5008965893299916\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bed7485df614fb4b07d32032ead966d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold-0] epoch-1: score=0.47667882705908005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99e7f294b0f4291a8c95f41aebea0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold-0] epoch-2: score=0.4633530388023636\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b14187388fe4c54a8b995a3486016f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(CFG.LOCAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5223d955-def9-40cd-be2d-aa91aa0c3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "!kaggle datasets init -p {CFG.OUTPUT}\n",
    "\n",
    "with open(f\"{CFG.OUTPUT}/dataset-metadata.json\", \"r\") as f:\n",
    "    d = json.load(f)\n",
    "    \n",
    "t = f\"FB3 {CFG.EXP} output\"\n",
    "d['title'] = t\n",
    "d['id'] = \"takamichitoda/\"+\"-\".join(t.split())\n",
    "\n",
    "with open(f\"{CFG.OUTPUT}/dataset-metadata.json\", \"w\") as f:\n",
    "    json.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b17b0-0218-410c-bf47-bf9e0d21abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets create -p {CFG.OUTPUT}\n",
    "#!kaggle datasets version -m \"test\" -p {CFG.OUTPUT}/\n",
    "\n",
    "!kaggle datasets list -m --sort-by \"updated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afc99225-e670-483d-891a-e309ba2e909e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'FB3 WLP Round.ipynb'\n",
      "'FB3 WLP mask loss.ipynb'\n",
      "'FB3 training WeightedLayerPooling.ipynb'\n",
      "'FB3 training multi GPU.ipynb'\n",
      " exp119\n",
      " exp189\n",
      " fb3-make-pseudo-label-4th\n",
      " fb3-pseudo-train-3rd.log\n",
      " feedback-prize-english-language-learning\n",
      " kaggle.json\n",
      " output\n",
      " pseudo\n",
      " pseudo_base\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ac2c0f-161f-49d1-8545-44533fdde191",
   "metadata": {},
   "source": [
    "# add exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3654a4-6756-4266-97d3-3a4c6b4ad021",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59011cb7-e41a-4702-bbc3-4b8ae9e59847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
